\chapter{Results}\label{chapter:results}
In this chapter, we present the results of our experiments.
Starting with a general overview, we show the effectiveness of the different LLMs on the retrieval-based implicit evaluation method.
After the overview, the results and visualizations needed to evaluate the research questions formulated in Section \ref{sec:research-question} are presented.

\section{Overview}
We use monoT5, which is our best performing pipeline to rank the generated answers of the LLMs against the human generated web documents in the dataset.
Because monoT5 performs best on all three metrics, we use it for all further experiments and only evaluate the one ranking produced by this pipeline.

In total, we rank 16000 generated answers, with each of the 8 models generating 10 answers for the 4 different prompting strategies for all 50 queries.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/weighted_position_boxplot.pdf}
\caption{Boxplot of the normalized position of the LLM answers, by model. }
\label{fig:weighted_position_boxplot}
\end{figure}
Figure \ref{fig:weighted_position_boxplot} shows the normalized position of the LLM answers in the ranking.
The normalized position is the absolute position of the answer in the ranking, divided by the total number of documents for the query.

The GPT-2 models generally perform worse, which was expected, as they are not fine-tuned for the task of question answering and are generally much smaller than the other models.
Interestingly, even those models have a number of first place rankings.
The four different sizes of GPT-2 nicely show the effect of model size on ranking results, with the median ranking position improving with each size increase.

Of the fine-tuned models, ChatGPT performs best with a median position of 0.006, scoring the best or second-best answer at nearly all queries.
Falcon 7B is the worst of the fine-tuned models with the Llama models performing slightly better.

ChatGPT is also the most consistent model, with the smallest standard deviation of 0.05.
The other models have much higher standard deviations, with the Llama models both being around 0.2 and the other models between 0.3 and 0.35.

\section{RQ1: Factors influencing LLM effectiveness}
In this section, we present results that help in investigating how different factors affect the performance of LLMs on the implicit evaluation method.
We look at the influence of model size, prompting strategy, answer length and query type on the ranking of the LLM answers.
Additionally, we investigate the answers generated by ChatGPT that are ranked especially low.

\subsection{Influence of Model Size}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/weighted_position_vs_model_size.pdf}
    \caption{Scatterplot showing number of model parameters against weighted normalized position of the model answers, as the mean over all prompts. Model size is shown on a logarithmic scale.}
    \label{fig:weighted_position_vs_model_size}
\end{figure}

Figure \ref{fig:weighted_position_vs_model_size} shows the weighted normalized position of the LLM answers plotted against the number of model parameters.
There is a clear trend of more model parameters leading to better rankings, with the GPT-2 models performing worst and the 185B parameters ChatGPT model performing best.
The 7B and 13B parameter models 



\subsection{Influence of Prompting Strategy}
The different prompting strategies strongly influence the final rankings of the LLM answers.
\begin{table}[tb]
\centering
\begin{tabular}{lllll}
\hline
\textbf{Model}        & \textbf{No Prompt} & \textbf{QA}     & \textbf{QuestionAnswer} & \textbf{MultiMedQA} \\\hline
GPT-2        & 0.763      & 0.663 & 0.614    & 0.603      \\
GPT-2 Medium & 0.675      & 0.524 & 0.544    & 0.618      \\
GPT-2 Large  & 0.496      & 0.393 & 0.336    & 0.374      \\
GPT-2 XL     & 0.509      & 0.336 & 0.327    & 0.292      \\
Falcon 7B    & 0.324      & 0.133 & 0.118    & 0.012      \\
Llama-2 7B   & 0.211      & 0.073 & 0.027    & 0.005      \\
Llama-2 13B  & 0.218      & 0.068 & 0.032    & 0.01       \\
ChatGPT      & 0.006      & 0.01  & 0.007    & 0.001     \\
\hline
\end{tabular}
\caption{Mean normalized position of the LLM answers, by prompting strategy.
The prompting strategies are described in section \ref{sec:prompting-approaches}, order here from least to most complex.
Increasing prompting complexity generally improves the ranking of the LLM answers for most models, with exception of GPT-2 Medium and GPT-2 Large.
}
\label{tab:prompting_strategy}
\end{table}
Table \ref{tab:prompting_strategy} shows the mean normalized position of the LLM answers for the different prompting strategies.
Starting with no prompt produces the worst ranking results for all models, except for ChatGPT where the QuestionAnswer prompt performs slightly worse.
The rankings improve with each more complex prompting strategy, except for GPT-2 Medium and GPT-2 Large, which perform best on the QA and the QuestionAnswer prompts respectively.
Even the three smaller instruction tuned models (Falcon and Llama) show strong improvements when going from no prompt at all to the simple QA prompt.

\subsection{Keyword vs Question Queries}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/weighted_position_boxplot_by_model_and_question.pdf}
    \caption{Boxplot of the normalized position of the LLM answers, by model and query type.}
    \label{fig:weighted_position_boxplot_by_model_and_question}
\end{figure}
Figure \ref{fig:weighted_position_boxplot_by_model_and_question} shows the normalized position of the LLM answers, split by query type and model.
Answers generated for the properly phrased question-type queries achieve higher median rankings than the answers generated for the keyword queries.
The spread of the rankings is also smaller for the question-type queries, with the keyword queries having more outliers at the bottom of the ranking.
This also holds true for ChatGPT, which is the most consistent model overall.

\subsection{Lower Ranked ChatGPT Answers}
While ChatGPT generally performs best, there are still some answers that are ranked badly.
We manually inspect each of the ChatGPT generated answers with a normalized ranking position of 0.1 or higher, meaning if there are 100 documents for the query, the answer is ranked 10 or lower.
All the low ranked answers stem from four queries, namely ``hypothyroidism symptoms'', ``List of multiple sclerosis symptoms'',  ``exercises for better posture'' and ``my risk for developing type 2 diabetes'' and follow one of two patterns.
The answers for the question about the personal risk of developing type 2 diabetes all contain the phrase ``As an AI'', as in the phrase ``I'm sorry, but as an AI language model, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation.''.
This is a commonly used phrase by ChatGPT, which indicates that the model is unable to answer the question.
The answers for the other three queries are all formatted as lists, either using enumeration or simple bullet points.
To investigate if answers formatted as lists are generally ranked lower, we identify all answers that are formatted as lists by searching for generated answers containing more than five non-empty lines, of which at least three start with either a dash or a number.
We also identify all answers containing the phrase ``As an AI''.
Table \ref{tab:badly_ranked_answers} shows the mean normalized ranking position of the LLM answers, split by whether they are formatted as lists.
\begin{table}[tb]
    \centering
    \begin{tabular}{lcc}
    \hline
    \textbf{Models} & \multicolumn{2}{c}{\textbf{List Style}} \\
    \cline{2-3} 
    & \textbf{Yes} & \textbf{No} \\
    \hline
    \multirow{2}{*}{GPT-2}        & N/A           & 0.6608  \\
                                  & (0)           & (2000) \\
    \multirow{2}{*}{GPT-2 Medium} & N/A           & 0.5902  \\
                                  & (0)           & (2000) \\
    \multirow{2}{*}{GPT-2 Large}  & 0.4365        & 0.3992  \\
                                  & (36)          & (1964) \\
    \multirow{2}{*}{GPT-2 XL}     & 0.4004        & 0.3658   \\
                                  & (22)          & (1978)  \\
    \multirow{2}{*}{Falcon 7B}    & 0.312         & 0.1436  \\
                                  & (38)          & (1962) \\
    \multirow{2}{*}{Llama-2 7B}   & 0.0416        & 0.0849  \\
                                  & (266)         & (1734) \\
    \multirow{2}{*}{Llama-2 13B}  & 0.0483        & 0.0909  \\
                                  & (427)         & (1573) \\
    \multirow{2}{*}{ChatGPT}      & 0.0128        & 0.0021  \\
                                  & (748)         & (1252) \\
    \hline
    \end{tabular}
    \caption{Mean normalized ranking position of the LLM answers, split by whether they are formatted as a list. The number in brackets is the number of answers in that category. If that number is 0, the model did not generate any answers in that format, so the mean is not available (N/A).}

    \label{tab:badly_ranked_answers}
\end{table}
ChatGPT generates the most answers formatted as lists, with 748 of the 2000 answers following that pattern.
On average, those answers are ranked worse than the other answers by ChatGPT, with a mean normalized ranking position of 0.0128 compared to 0.0021 for the non list answers.
The other models do not generate as many answers formatted as lists, with the both Llama models generating the most, 427 and 266 respectively.
The answers by Llama models which are formatted as lists score higher than the non-list ones, while for the remaining models the answer rank follows the ChatGPT patter, in that list-style answers are ranked lower than non list-style answers.

For ChatGPT, the answers containing the phrase ``As an AI'' are ranked worse than the other answers, with a mean normalized ranking position of 0.0578 compared to 0.0056.
The only other model frequently generating that phrase is the Falcon 7B model.
With a mean normalized rank of 0.191 the answers containing the phrase also rank worse, but the difference is not as pronounced compared to the rank of 0.146 which the remaining answers by Falcon 7B achieve.

\section{RQ2: Comparison to other Benchmarks}\label{sec:benchmark_comparison}
We compare the median normalized ranking position of the LLM answers to the results of other benchmarks.
To our knowledge, there are currently no benchmarks available that evaluate the effectiveness of LLMs for LFQA and provide results for the same models that we use.
The HuggingFace Open LLM Leaderboard by \cite{beeching:2023:Open} provides results on multiple different benchmarks for many models hosted on the HuggingFace hub.
From here, we select the ARC~(\cite{clark:2018:Think}), the HellaSwag~(\cite{zellers:2019:HellaSwag}) and the MMLU~(\cite{hendrycks:2020:Measuring}) datasets, all of which are single choice question answering benchmarks.
\begin{table}[tb]
    \centering
    \begin{tabularx}{\textwidth}{lcccc}
    \hline
    \textbf{Model} & \textbf{Mean Ranking} & \textbf{ARC}  & \textbf{HellaSwag} & \textbf{MMLU} \\\hline
    GPT-2          & 0.661                             & 21.8          & 31.6               & 25.9          \\
    GPT-2 Medium   & 0.59                              & 27.0          & 40.2               & 26.6          \\
    GPT-2 Large    & 0.4                               & 25.9          & 45.6               & 26.1          \\
    GPT-2 XL       & 0.366                             & 30.3          & 51.4               & 26.4          \\
    Falcon 7B      & 0.147                             & 46.2          & 70.9               & 25.8          \\
    Llama-2 13B    & 0.082                             & 59.0          & 81.9               & 54.6          \\
    Llama-2 7B     & 0.079                             & 52.9          & 78.6               & 48.3          \\
    ChatGPT        & \textbf{0.006}                    & \textbf{85.2} & \textbf{85.5}      & \textbf{70.0} \\
    \hline
    \end{tabularx}
    \caption{Comparison of the mean normalized ranking position of the LLM answers to the results of other benchmarks.
    For the HuggingFace models, those scores are taken from the HuggingFace Open LLM Leaderboard by \cite{beeching:2023:Open}.
    For ChatGPT, we estimate the score from the GPT-4 Technical Report~(\cite{openai:2023:GPT}), which provides scores for GPT-3.5, the underlying model of ChatGPT.
    This is not the exact model we use in out testing, but we assume that the capabilities are similar.
    Mean Ranking is the mean normalized ranking position over all generated answers for the model.
    }
    \label{tab:benchmark_comparison}
\end{table}
Table \ref{tab:benchmark_comparison} shows the scores over those benchmarks, as well as the mean normalized ranking position of the LLM answers.
On all benchmarks, there is the general trend of higher number of model parameters leading to better scores.
For our dataset this trend is only broken for the two Llama versions, with the 7B model ranking slightly better than the 13B model.
Other than that, our results are consistent with the results of the traditional benchmarks.
