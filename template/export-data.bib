
@misc{singhal_large_2022,
	title = {Large {Language} {Models} {Encode} {Clinical} {Knowledge}},
	url = {http://arxiv.org/abs/2212.13138},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and Arcas, Blaise Aguera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13138 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{nori_capabilities_2023,
	title = {Capabilities of {GPT}-4 on {Medical} {Challenge} {Problems}},
	url = {http://arxiv.org/abs/2303.13375},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.},
	urldate = {2023-06-29},
	publisher = {arXiv},
	author = {Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
	month = apr,
	year = {2023},
	note = {arXiv:2303.13375 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
}

@inproceedings{meel_web_2020,
	title = {Web {Text} {Content} {Credibility} {Analysis} using {Max} {Voting} and {Stacking} {Ensemble} {Classifiers}},
	doi = {10.1109/ACCTHPA49271.2020.9213234},
	abstract = {The social media has become a great medium for people around the world to openly express their thoughts and views. But for all its advantages, it has also paved way for many people and organizations to intentionally spread fake news and misinform others. And the rate at which fake news is being currently generated, it has become critical to create a reliable mechanism that can efficiently classify a real news from a fake one. This research paper analyses the different approaches, involving ensemble learning, that can be used to accomplish the same by using only text features of the news data. We observe that a combination of three optimal ML algorithms, clubbed by an advanced ensemble learning technique, can give results with an accuracy of more than ninety eight percent.},
	booktitle = {2020 {Advanced} {Computing} and {Communication} {Technologies} for {High} {Performance} {Applications} ({ACCTHPA})},
	author = {Meel, Priyanka and Chawla, Puneet and Jain, Sahil and Rai, Utkarsh},
	month = jul,
	year = {2020},
	keywords = {Analytical models, Logistics, Machine learning, Measurement, Radio frequency, Stacking, Support vector machines, bernoulli naïve bayes, ensemble, fake news detection, k-nearest neighbors, logistic regression, notion, random forest, svm},
	pages = {157--161},
}

@misc{imperial_bert_2021,
	title = {{BERT} {Embeddings} for {Automatic} {Readability} {Assessment}},
	url = {http://arxiv.org/abs/2106.07935},
	abstract = {Automatic readability assessment (ARA) is the task of evaluating the level of ease or difficulty of text documents for a target audience. For researchers, one of the many open problems in the field is to make such models trained for the task show efficacy even for low-resource languages. In this study, we propose an alternative way of utilizing the information-rich embeddings of BERT models with handcrafted linguistic features through a combined method for readability assessment. Results show that the proposed method outperforms classical approaches in readability assessment using English and Filipino datasets, obtaining as high as 12.4\% increase in F1 performance. We also show that the general information encoded in BERT embeddings can be used as a substitute feature set for low-resource languages like Filipino with limited semantic and syntactic NLP tools to explicitly extract feature values for the task.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Imperial, Joseph Marvin},
	month = jul,
	year = {2021},
	note = {arXiv:2106.07935 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@article{mitra_credbank_2015,
	title = {{CREDBANK}: {A} {Large}-{Scale} {Social} {Media} {Corpus} {With} {Associated} {Credibility} {Annotations}},
	volume = {9},
	copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
	issn = {2334-0770},
	shorttitle = {{CREDBANK}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14625},
	doi = {10.1609/icwsm.v9i1.14625},
	abstract = {Social media has quickly risen to prominence as a news source, yet lingering doubts remain about its ability to spread rumor and misinformation. Systematically studying this phenomenon, however, has been difficult due to the need to collect large-scale, unbiased data along with in-situ judgements of its accuracy. In this paper we present CREDBANK, a corpus designed to bridge this gap by systematically combining machine and human computation.  Specifically, CREDBANK is a corpus of tweets, topics, events and associated human credibility judgements. It is based on the real-time tracking of more than 1 billion streaming tweets over a period of more than three months, computational summarizations of those tweets, and intelligent routings of the tweet streams to human annotators — within a few hours of those events unfolding on Twitter. In total CREDBANK comprises more than 60 million tweets grouped into 1049 real-world events, each annotated by 30 human annotators. As an example, with CREDBANK one can quickly calculate that roughly 24\% of the events in the global tweet stream are not perceived as credible. We have made CREDBANK publicly available, and hope it will enable new research questions related to online information credibility in fields such as social science, data mining and health.},
	language = {en},
	number = {1},
	urldate = {2023-06-17},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Mitra, Tanushree and Gilbert, Eric},
	year = {2015},
	note = {Number: 1},
	keywords = {Micro labor annotations, notion},
	pages = {258--267},
}

@inproceedings{tanaka_evaluating_2010,
	address = {New York, NY, USA},
	series = {{ICUIMC} '10},
	title = {Evaluating credibility of web information},
	isbn = {978-1-60558-893-3},
	url = {https://doi.org/10.1145/2108616.2108645},
	doi = {10.1145/2108616.2108645},
	abstract = {We describe a new concept and method for evaluating the Web information credibility. The quality control of information (text, image, video etc.) on the Web is generally insufficient due to low publishing barriers. As a result, there is a large amount of mistaken and unreliable information on the Web that can have detrimental effects on users. This calls for technology that facilitates the judging of the credibility (expertise and trustworthiness) of Web content and the accuracy of the information that users encounter on the Web. Such technology should be able to handle a wide range of tasks: extracting several credibility-related features from the target Web content, extracting reputation-related information for the target Web content, such as hyperlinks and social bookmarks and evaluating its distribution, and evaluating features of the target content authors. We propose and describe methodologies of analyzing information credibility of Web information: (1) content analysis, (2) social support analysis and (3) author analysis. We overview our recent research activities on Web information credibility evaluation based on this methodologies.},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Uniquitous} {Information} {Management} and {Communication}},
	publisher = {Association for Computing Machinery},
	author = {Tanaka, Katsumi and Ohshima, Hiroaki and Jatowt, Adam and Nakamura, Satoshi and Yamamoto, Yusuke and Sumiya, Kazutoshi and Lee, Ryong and Kitayama, Daisuke and Yumoto, Takayuki and Kawai, Yukiko and Zhang, Jianwei and Nakajima, Shinsuke and Inagaki, Yoichi},
	month = jan,
	year = {2010},
	keywords = {credibility, trustworthiness, web search},
	pages = {1--10},
}

@article{ermakova_overview_nodate,
	title = {Overview of the {CLEF} 2022 {SimpleText} {Task} 2: {Complexity} {Spotting} in {Scientific} {Abstracts}},
	abstract = {This paper provides an overview of the Task 2: What is unclear? of the Automatic Simplification of Scientific Texts (SimpleText) lab, run as part of CLEF 2022. The main aim of the SimpleText lab is to promote a more open scientific information access via automatic text simplification. Task 2 focuses on complexity spotting within scientific texts (passage). Thus, the goal is to detect the terms/concepts that require specific background knowledge for understanding of the passage and to assess their complexity for non-experts. Overall, four runs from four different teams have been submitted to this task. In this paper, we describe the data collection, the task setup, and the evaluation procedure. We also give a brief overview of the participating approaches.},
	language = {en},
	author = {Ermakova, Liana and Ovchinnikov, Irina and Kamps, Jaap and Nurbakova, Diana and Araújo, Sílvia and Hannachi, Radia},
	keywords = {notion},
}

@inproceedings{ranasinghe_transquest_2020,
	address = {Barcelona, Spain (Online)},
	title = {{TransQuest}: {Translation} {Quality} {Estimation} with {Cross}-lingual {Transformers}},
	shorttitle = {{TransQuest}},
	url = {https://aclanthology.org/2020.coling-main.445},
	doi = {10.18653/v1/2020.coling-main.445},
	abstract = {Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},
	month = dec,
	year = {2020},
	keywords = {notion},
	pages = {5070--5081},
}

@inproceedings{goeuriot_consumer_2021,
	title = {Consumer {Health} {Search} at {CLEF} {eHealth} 2021},
	url = {https://www.semanticscholar.org/paper/Consumer-Health-Search-at-CLEF-eHealth-2021-Goeuriot-Suominen/49795bd68af14c6c6970ce68aafcfe9d61a076fa},
	abstract = {This paper details materials, methods, results, and analyses of the Consumer Health Search Task of the CLEF eHealth 2021 Evaluation Lab. This task investigates the effectiveness of information retrieval (IR) approaches in providing access to medical information to laypeople. For this a TREC-style evaluation methodology was applied: a shared collection of documents and queries is distributed, participants’ runs received, relevance assessments generated, and participants’ submissions evaluated. The task generated a new representative web corpus including web pages acquired from a 2021 CommonCrawl and social media content from Twitter and Reddit, along with a new collection of 55 manually generated layperson medical queries and their respective credibility, understandability, and topicality assessments for returned documents. This year’s task focused on three subtask: (i) ad-hoc IR, (ii) weakly supervised IR, and (iii) document credibility prediction. In total, 15 runs were submitted to the three subtasks: eight addressed the ad-hoc IR task, three the weakly supervised IR challenge, and 4 the document credibility prediction challenge. As in previous years, the organizers have made data and tools associated with the task available for future research and development.},
	urldate = {2023-06-15},
	author = {Goeuriot, L. and Suominen, H. and Pasi, G. and Bassani, Elias and Brew-Sam, N. and Sáez, Gabriela Nicole González and Kelly, L. and Mulhem, P. and Seneviratne, Sandaru and Upadhyay, Rishabh and Viviani, Marco and Xu, Chenchen},
	year = {2021},
	keywords = {notion},
}
