\chapter{Experimental Setup}
This chapter describes the experimental setup of this thesis, including the data collection and preparation, the retrieval pipeline development and the generation of LLM responses.

\section{Overview}\label{sec:overview}
The final goal of this project is to have a pipeline with which the answers of different LLMs can be ranked, in comparison to human answers.
This means, we first need a dataset, consisting of questions as well as human answers to these questions.
Those human generated answers need to be ranked according to how well they answer the given question.
Then, we need to develop a retrieval pipeline, which is able to retrieve the best answer to a given question in accordance to the ranking of the human answers.
Finally, the answers generated by different LLMs can be ranked using the developed retrieval pipeline.
\\\\
This allows us to compare long form question answering capabilities between different LLMs or between different prompting strategies for the same LLM.
Additionally, an expected range of performance for a given LLM can be established, given that the answer of a LLM differs every time it is prompted with the same question.
\\\\
To be able to do this, we first need to collect a suitable dataset, since a benchmark like this has not been formulated before.
Then, different retrieval methods have to be compared on the dataset, to find the best performing one.
Finally, the LLMs have to be selected and the answers generated by them have to be ranked using the best retrieval method.
\\
This process is described in detail in the following sections.

\section{Data Collection and Preparation}
The used dataset is based on the test set of the CLEF eHealth 2021 dataset \cite{goeuriot:2021}.
It was originally intended to evaluate the ability of retrieval systems to provide credible, readable and relevant answers to laypersons' health questions.

\subsection{CLEF eHealth 2021 Dataset}
The test set consists of 50 health related queries, which either stem from Reddit or Google search trends.
While the Reddit-sourced queries are well-formulated questions about specific health topics, the queries from Google search trends are not necessarily phrased as questions but rather as classical search queries.
\\
In addition to the queries, the dataset includes a collection of web documents and social media content.
The web documents were mainly obtained from the CommonCrawl archive, encompassing a diverse range of 600 domains.
This list of domains was created by executing medical queries via Microsoft Bing API and was augmented by adding known reliable and unreliable health-related websites.
The dataset was expanded by incorporating social media comments and posts from Reddit and Twitter.
These were collected by manually generating search queries based on 150 pre-selected health topics and retrieving relevant posts and comments from these platforms.
\\
To get evaluations in each of the three categories (credible, readable and relevant), each query was assigned 250 documents based on rank-biased precision (RBP)-based Method A~(\cite{moffat:2008}).
RBPA is a method for choosing which documents from a pool of documents to select for evaluation by human annotators.
The pool of documents in this case is the collection of web documents and social media content returned by the participating teams of the shared task, as well as the organizer's baseline systems.
From this, documents are evaluated based on their rank in different runs, and then scored according to different metrics.
A more detailed description of the pooling method can be found in \cite{lipani:2017}.
\\
After the 250 documents per query are selected, the documents are evaluated by humans for credibility, readability and relevance.
In total, there are 26 annotators, each annotating documents for between 1 and 4 queries completely.
The annotators were not medical experts, but received written training material.
In the end, annotations were made for  a total of $11 357$ unique documents.
This differs from the total amount of annotations which is $12 500$, since some documents were annotated multiple times but for different queries.
\\
The annotations for relevance and readability are in the categories 0 (not relevant/readable), 1 (somewhat relevant/readable) and 2 (highly relevant/credible).
For scoring credibility, a category 3 exists, which is also interpreted as highly credible.
A reasoning for this is not given in the original paper.
\\\\
How the adapted dataset is created from the original dataset is described in the following section.
\subsection{Dataset Collection}
There are two ways of accessing the CLEF eHealth 2021 dataset.
\\
Option one is to download the indexed collection directly.
This index does not contain the full text of the documents, so it can't be used to compare the original documents against newly generated answers.
\\
The second option is to access all documents individually, given their IDs.
This is in easily done in theory, by using the provided scripts in the organizer's GitHub repository\footnote{\url{https://github.com/CLEFeHealth/CHS-2021}}.
Scripts are provided to download the documents from the CommonCrawl archive, as well as the social media content directly over the respective APIs.
\\
Unfortunately, with Twitter shutting down their free tier of the API, in the beginning of April 2023, this is now only possible with a prohibitively expensive paid tier.
\\
For Reddit, those API changes followed just a few months later, making it impossible to download the social media content from Reddit as well.
Even tough we started downloading some Reddit content before those changes, the already slow access to the Reddit data did not allow for downloading all content in time.
\\
This was unfortunate timing, but since the organizers of the CLEF eHealth task reported in their paper that the human credibility assessments of social media documents were significantly lower than those of web documents, we decided to only use the web documents for our dataset.
Those are easily accessible via the CommonCrawl archive.
\\
\\
Before accessing all relevant documents from the CommonCrawl archives, the list of documents was filtered to only include documents which were annotated for at least one query.
Discarding all documents from Reddit and Twitter, as well as web documents which were not assessed for any query, resulted in a total of $6692$ documents downloaded from the CommonCrawl archive using a slightly modified version of the provided script.
This provides us with WARC files for the relevant documents.

\subsection{Preprocessing}
The files in the WARC format contain the full HTML of the web documents.
Before being able to compare the content to the generated answers, the HTML is extracted from the documents.
HTML elements containing fewer than 50 characters are discarded, assuming they are not relevant for the content of the document (e.g. navigation bars).
Then, the plain text is extracted using the ChatNoir Resiliparse Library\footnote{\url{https://resiliparse.chatnoir.eu/en/stable/}}.


\subsection{Dataset Statistics}


\section{Retrieval Pipeline Development}
In this section, the implementation of the different retrieval pipelines is discussed.
All models are implemented in the PyTerrier Framework~(\cite{pyterrier:2020}), which is a Python API for the Terrier IR Platform~(\cite{macdonald:2012}).

\subsection{Baseline Models}

\subsection{Transformer Models}

\subsection{Addition of external scores}

\section{Generation of LLM Responses}


\subsection{Selection of Language Models}

\subsection{Prompting approaches}
