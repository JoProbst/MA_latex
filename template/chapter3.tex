\chapter{Experimental Setup}
This chapter describes the experimental setup of this thesis, including the data collection and preparation, the retrieval pipeline development and the generation of LLM responses.

\section{Overview}\label{sec:overview}
The final goal of this project is to have a pipeline with which the answers of different LLMs can be ranked, in comparison to human answers.
This means, we first need a dataset, consisting of questions as well as human answers to these questions.
Those human generated answers need to be ranked according to how well they answer the given question.
Then, we need to develop a retrieval pipeline, which is able to retrieve the best answer to a given question in accordance to the ranking of the human answers.
Finally, the answers generated by different LLMs can be ranked using the developed retrieval pipeline.
\\\\
This allows us to compare long form question answering capabilities between different LLMs or between different prompting strategies for the same LLM.
Additionally, an expected range of performance for a given LLM can be established, given that the answer of a LLM differs every time it is prompted with the same question.
\\\\
To be able to do this, we first need to collect a suitable dataset, since a benchmark like this has not been formulated before.
Then, different retrieval methods have to be compared on the dataset, to find the best performing one.
Finally, the LLMs have to be selected and the answers generated by them have to be ranked using the best retrieval method.
\\
This process is described in detail in the following sections.

\section{Data Collection and Preparation}
The used dataset is based on the test set of the CLEF eHealth 2021 dataset \cite{goeuriot:2021}.
It was originally intended to evaluate the ability of retrieval systems to provide credible, readable and relevant answers to laypersons' health questions.

\subsection{CLEF eHealth 2021 Dataset}
The test set consists of 55 health related queries, which either stem from Reddit or Google search trends.
While the Reddit-sourced queries are well-formulated questions about specific health topics, the queries from Google search trends are not necessarily phrased as questions but rather as classical search queries.
\\
In addition to the queries, the dataset includes a collection of web documents and social media content.
The web documents were mainly obtained from the CommonCrawl archive, encompassing a diverse range of 600 domains.
This list of domains was created by executing medical queries via Microsoft Bing API and was augmented by adding known reliable and unreliable health-related websites.
The dataset was expanded by incorporating social media comments and posts from Reddit and Twitter.
These were collected by manually generating search queries based on 150 pre-selected health topics and retrieving relevant posts and comments from these platforms.
\\
To get evaluations in each of the three categories (credible, readable and relevant), each query was assigned 250 documents based on rank-biased precision (RBP)-based Method A~(\cite{moffat:2008}).
RBPA is a method for choosing which documents from a pool of documents to select for evaluation by human annotators.
The pool of documents in this case is the collection of web documents and social media content returned by the participating teams of the shared task, as well as the organizer's baseline systems.
From this, documents are evaluated based on their rank in different runs, and then scored according to different metrics.
A more detailed description of the pooling method can be found in \cite{lipani:2017}.
\\
After the 250 documents per query are selected, the documents are evaluated by humans for credibility, readability and relevance.
In total, there are 26 annotators, each annotating documents for between 1 and 4 queries completely.
The annotators were not medical experts, but received written training material.
In the end, annotations were made for  a total of $11 357$ unique documents.
This differs from the total amount of annotations which is $12 500$, since some documents were annotated multiple times but for different queries.
\\
The annotations for relevance and readability are in the categories 0 (not relevant/readable), 1 (somewhat relevant/readable) and 2 (highly relevant/credible).
For scoring credibility, a category 3 exists, which is also interpreted as highly credible.
A reasoning for this is not given in the original paper.
\\\\
How the adapted dataset is created from the original dataset is described in the following section.
\subsection{Dataset Collection}
There are two ways of accessing the CLEF eHealth 2021 dataset.
\\
Option one is to download the indexed collection directly.
This index does not contain the full text of the documents, so it can't be used to compare the original documents against newly generated answers.
\\
The second option is to access all documents individually, given their IDs.
This is in easily done in theory, by using the provided scripts in the organizer's GitHub repository\footnote{\url{https://github.com/CLEFeHealth/CHS-2021}}.
Scripts are provided to download the documents from the CommonCrawl archive, as well as the social media content directly over the respective APIs.
\\
Unfortunately, with Twitter shutting down their free tier of the API, in the beginning of April 2023, this is now only possible with a prohibitively expensive paid tier.
\\
For Reddit, those API changes followed just a few months later, making it impossible to download the social media content from Reddit as well.
Even tough we started downloading some Reddit content before those changes, the already slow access to the Reddit data did not allow for downloading all content in time.
\\
This was unfortunate timing, but since the organizers of the CLEF eHealth task reported in their paper that the human credibility assessments of social media documents were significantly lower than those of web documents, we decided to only use the web documents for our dataset.
The web documents are easily accessible via the CommonCrawl archive.
\\
\\
Before accessing all relevant documents from the CommonCrawl archives, the list of documents was filtered to only include documents which were annotated for at least one query.
Discarding all documents from Reddit and Twitter, as well as web documents which were not assessed for any query, resulted in a total of $6692$ documents downloaded from the CommonCrawl archive using a slightly modified version of the provided script.
This provides us with WARC files for the relevant documents.
\\
Since all social media content was discarded, the number of queries for which documents are available is reduced to 50, since of the total 55 queries, 5 queries only have social media content associated with them.

\subsection{Preprocessing}
The files in the WARC format contain the full HTML of the web documents.
Before being able to compare the content to the generated answers, the HTML is extracted from the documents.
HTML elements containing fewer than 50 characters in the elements body are discarded, assuming they are not relevant for the content of the document (e.g. fields of navigation bars).
Then, the plain text is extracted using the ChatNoir Resiliparse Library\footnote{\url{https://resiliparse.chatnoir.eu/en/stable/}}.
\\
The final dataset now consists of 50 queries, each of which has between 39 and 249 documents associated with it that are evaluated for relevance, readability and credibility.


\section{Retrieval Pipeline Development}
In this section, the implementation of the different retrieval pipelines is discussed.
The baseline models (DPH and TF-IDF), as well as the ColBERT version 1 and the monoT5/duoT5 based pipelines are implemented in the PyTerrier framework by \cite{pyterrier:2020}, which is a Python API for the Terrier IR Platform~(\cite{macdonald:2012}).
The ColBERT version 2 pipeline is based on the original implementation provided by the authors\footnote{\url{https://github.com/stanford-futuredata/ColBERT}}.
\\
Those pipelines are all designed to retrieve the most relevant documents for a given query.
Since our dataset also contains annotations for readability and credibility, we also want to be able to retrieve documents based on those criteria.
To extend the retrieval pipelines for those metrics, we experiment with adding external scores to the retrieval process.
This is described in section \ref{sec:external-scores}.

\subsection{Retrieval Setup}
The retrieval setup in this work differs from the usual retrieval setup, in which a large document collection is indexed, and then multiple queries are run against the index using different retrieval models.
In our case, we build separate indices for each query, containing only the documents that are evaluated for that query.
This ensures that each document that is retrieved for a query is also evaluated for that query.

\subsection{Baseline Models}
\begin{figure}
    \begin{tikzpicture}[node distance=3cm, every node/.style={scale=0.7}]
    \tikzstyle{box} = [rectangle, draw, fill=blue!10,  text width=2cm, text centered, rounded corners, minimum height=3em]
    \tikzstyle{process} = [rectangle, draw, fill=blue!30,  text width=2cm, text centered, rounded corners, minimum height=3em]
    \node (docCol) [box] {Document Collection};
    \node (preprocess) [process, right of=docCol] {Pre-\\processing};
    \node (invertedindex) [process, right of=preprocess] {Inverted Index};
    \node (retrieval) [process, right of=invertedindex] {Retrieval Model (TF-IDF / DPH)};
    \node (qe) [process, right of=retrieval] {Query Expansion};
    \node (reranking) [process, right of=qe] {Retrieval Model (TF-IDF / DPH)};
    \node (ranked) [box, right of=reranking] {Ranked Documents};
    \node (query) [box, above of=retrieval] {Query};

    \draw [->] (docCol) -- (preprocess);
    \draw [->] (preprocess) -- (invertedindex);
    \draw [->] (invertedindex) -- (retrieval);
    \draw [->] (retrieval) -- (qe);
    \draw [->] (qe) -- (reranking);
    \draw [->] (reranking) -- (ranked);
    \draw [->] (query) -- (retrieval);

    \end{tikzpicture}
\caption{Baseline retrieval pipeline, using either TF-IDF or DPH as retrieval model as well as query expansion.}
\label{fig:baseline-pipeline}
\end{figure}


This section describes the implementation of the baseline models, namely DPH and TF-IDF.
For both models, the basic indexing function of PyTerrier is used, which indexes and preprocesses the documents.
Preprocessing is done with the default values, which includes the following operations:
\begin{itemize}
    \item{\textbf{Tokenization}, using the default PyTerrier tokenizer, which splits on non-alphanumeric characters. Additional rules to discard tokens which are longer than 20 characters, contain more than 4 digits, or contain the same character more than 3 times in a row are applied. All tokens are also converted to lowercase.}
    \item \textbf{Stopword removal}, using the default PyTerrier stopword list.
    \item \textbf{Stemming}, using the Porter stemmer, a rule-based stemmer.
\end{itemize}
The resulting inverted index contains all remaining tokens, and a mapping from each token to the documents in which it occurs.
\\
The two retrieval models TF-IDF and DPH are then applied using the default parameters.
More details about the models can be found in section \ref{sec:baseline-retrieval-models}.
\\
\\
Both models are tested with and without query expansion.
We use BO1 query expansion, which is a query expansion method that uses the documents retrieved by the original query to expand the query.
It adds terms from the retrieved documents to the original query based on informativeness, which is a measure of how frequently the term occurs in the documents compared to how frequently it occurs in the whole collection.
Terms that occur more frequently in the retrieved documents than in the collection are added to the query.
Afterwards, the expanded query is run against the index again, and the documents are ranked according to the new query.
\\
\\
The resulting pipeline is shown in figure \ref{fig:baseline-pipeline}.
This produces a ranked list of documents for each query, which can then be compared to the order of documents according to the human annotations.

\subsection{Transformer Models}
In addition to the baseline models, we also implement the transformer based models ColBERT version 1 and 2, as well as the monoT5 and duoT5 based models.
The implementation of ColBERT version 1 and the MonoT5/DuoT5 based models is done in PyTerrier, while the implementation of ColBERT version 2 is done in the original implementation provided by the authors.
\\\\
Since all of those models relay on a pre-trained model, the first step is to select the model to use.
The most commonly found pre-trained models are trained on the MS MARCO passage ranking dataset~(\cite{bajaj:2016}), a large scale dataset for passage retrieval.
This is the dataset on which ColBERT version 1 and the MonoT5/DuoT5 based models are trained.
For ColBERT version 2, two different pre-trained models are evaluated, one trained on the MS MARCO passage ranking dataset, and one trained on the p

\subsubsection{ColBERT Version 1}
Using the ColBERT implementation for PyTerrier\footnote{\url{https://github.com/terrierteam/pyterrier_colbert}}, an end-to-end pipeline can be constructed from a pre-trained ColBERT model and a document collection.
In our testing, we use the pre-trained ColBERT checkpoint provided by the authors.
\\
The checkpoint can directly be loaded in the PyTerrier framework, and then used to retrieve documents from the collection.
All settings are left at their default values.

\subsubsection{MonoT5 and DuoT5}
Like the ColBERT implementation, the MonoT5 and DuoT5 implementations are also available in PyTerrier\footnote{\url{https://github.com/terrierteam/pyterrier_t5}}.
Pre-trained checkpoints based on the MS MARCO dataset are provided by the authors.
\\
The MonoT5 pipeline directly uses the pre-trained checkpoint to rank the documents for each query.
\\
The DuoT5 pipeline is based on the MonoT5 pipeline, but uses the DuoT5 reranker to rerank the top 10 documents retrieved by the MonoT5 pipeline.
\\
Again, all parameters are left at their default values.

\subsubsection{ColBERT Version 2}
Unlike the other transformer based models, ColBERT version 2 is not available in PyTerrier.
Instead, we use the implementation provided by the authors of the model\footnote{\url{https://github.com/stanford-futuredata/ColBERT/tree/main}}.
\\
The implementation works with any pre-trained checkpoint for the ColBERT version 2 model, with the MS MARCO checkpoint being downloaded by default.

\subsection{External scores for Understandability}\label{sec:external-scores}
To improve retrieval performance for the understandability, we experiment with adding pre-computed external scores to the retrieval process.

To estimate understandability of the documents in the dataset, different scores are calculated for each document.
\\
The first score is the Flesch Reading Ease(FRE) score devised by \cite{kincaid:1975}, which is a score between 0 and 100, with higher scores indicating easier to read text.
It is calculated using the following formula, based on average sentence length(ASL) and average number of syllables per word(ASW):
\begin{equation}
    FRE = 206.835 - (1.015 \times ASL) - (84.6 \times ASW)
\end{equation}
We use the implementation provided by the textstat library\footnote{\url{https://pypi.org/project/textstat/}}.
\\
This library also provides the second score, which is an aggregate of multiple similar scores, like the FOG score, the SMOG score and the Coleman-Liau Index.
Those scores are all based on different text statistics, like the number of syllables per word, the number of words per sentence or the number of characters per word.
More details about the different scores can be found in the documentation of the textstat library.
\\

\section{Generation of LLM Responses}


\subsection{Selection of Language Models}

\subsection{Prompting approaches}
