\chapter{Discussion \& Limitations}\label{discussion}

This section provides an analysis of the experimental setup and results, investigating the quality of the dataset, the performances of various retrieval pipelines, and the capabilities of different LLMs on our dataset.
We will also discuss the limitations and potential biases inherent in our methodologies and results.

\section{RQ1: LLM factors}

This could potentially be due to the more sophisticated fine-tuning techniques for the Llama models, which are described in section \ref{sec:dialog-models}.
Between the two Llama models, the smaller one is slightly better, but they perform very similar.
This is not consistent with other benchmarks, where the larger model usually performs better, as shown in the following subsection \ref{sec:benchmark_comparison}.

Question vs. query
As those models are already fine-tuned on the task of question answering, we could have expected them to perform better on the no prompt strategy.
The biggest model, ChatGPT, shows the smallest differences in scores between the different prompting strategies, but still performs best with the MultiMedQA prompt.

The results are consistent with the findings of \cite{reynolds:2021:Prompt} that more complex prompting strategies improve the capabilities of LLMs.

list:
Here, the list answers are ranked higher than the remaining answers, so we can not assume that the worse ranking of list answers is a general pattern.
\section{RQ2}

\section{Dataset Properties and Challenges}
\begin{itemize}
    \item small dataset, only health
    \item all web data, noisy due to preprocessing
    \item annotations are subjective, inconsistent between mutliple ratings
    \item question type queries probably more relevant for LLMs, keyword based ones could be rewritten
\end{itemize}

\section{Retrieval Pipeline Limitations}
\begin{itemize}
    \item No big improvements of transformer over baseline, QE not making a huge difference
    \item relevance seems to increase the least with more complex model/not much difference between architectures
    \item baseline rankings closest to readability ranking of humans, would have expected relevance.
    \item Worst on credibility, because no built in way of judging this
    \item larger improvements in credibility and readability in transformer models
    \item We attribute this to the fact that the transformer-based models do not consider the documents as bags of words, but instead use the full text of the documents.
    \item This is especially important for readability, as the models can now take into account the structure of the text, including stopwords, punctuation, and other elements that are removed for the baseline models.
    \item Even though the transformer models are not specifically trained to consider readability and credibility, the relevant passages in the MS MARCO corpus seem to reflect those properties.
    \item can monoT5 still be seen as classical retrieval model? There is no indexing or direct comparison of answers, just the calculation of the score based on MS MARCO
    \item not much different from generating scores, like in the fine-tuning step of LLMs
    \item no specific mechanics for evaluating readability/credibility
    \item did not consider efficiency vs effectiveness, because effectiveness is most important
\end{itemize}


