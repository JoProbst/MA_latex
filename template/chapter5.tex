\chapter{Discussion \& Limitations}

This section provides an analysis of the experimental setup and results, investigating the quality of the dataset, the performances of various retrieval pipelines, and the capabilities of different LLMs on our dataset.
We will also discuss the limitations and potential biases inherent in our methodologies and results.
\section{Dataset Properties and Challenges}
\begin{itemize}
    \item small dataset, only health
    \item all web data, noisy due to preprocessing
    \item annotations are subjective, inconsistent between mutliple ratings
    \item question type queries probably more relevant for LLMs, keyword based ones could be rewritten
\end{itemize}
\section{Retrieval Pipeline Performances}
\begin{itemize}
    \item No big improvements of transformer over baseline, QE not making a huge difference
    \item relevance seems to increase the least with more complex model/not much difference between architectures
    \item larger improvements in credibility and readability in transformer models
    \item can monoT5 still be seen as classical retrieval model? There is no indexing or direct comparison of answers, just the calculation of the score based on MS MARCO
    \item not much different from generating scores, like in the fine-tuning step of LLMs
    \item no specific mechanics for evaluating readability/credibility
\end{itemize}

\section{Performance of Large Language Models}
% The performance of various LLMs, including GPT-2, Falcon, Llama, and ChatGPT, offers valuable insights into the capabilities and limitations of these models.
% Notably, ChatGPT consistently outperforms other models across different query types and prompting strategies.
% This superiority could be attributed to its advanced fine-tuning techniques and larger model size, which likely enable a better understanding of complex queries and more accurate answer generation.

% However, the performance variation among LLMs, especially when comparing different model sizes and fine-tuning techniques, underscores the complexity of long-form question answering.
% It is evident that larger and more sophisticated models tend to perform better, but this is not a universal rule, as seen with the Llama models.
\begin{itemize}
    \item ChatGPT outperforms other models significantly
    \item Much larger, probably sophisticated rewriting of given prompt
    \item Larger models tend to perform better, but not always (Llama)
    \item Prompting strategies have a large impact on performance
    \item Other benchmarks produce similar results
    \item 
\end{itemize}

\section{Limitations and Future Directions}
\begin{itemize}
    \item 
\end{itemize}
