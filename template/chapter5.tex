\chapter{Discussion}\label{discussion}

The main goal of this thesis was to investigate whether human-written web content from the medical domain can be used as a proxy for evaluating LLMs in LFQA.
In this chapter, we discuss the results of the underlying research questions that are investigated to answer this overarching question.
Finally, we discuss the limitations of our work.

\section{Factors Influencing LLM Effectiveness}

The effectiveness of LLMs in providing health-related answers varies significantly depending on several factors, as shown by our experimental results.

\subsection{Model Size}
Our results show a strong connection between model size and ranking results in our rank-based implicit evaluation as evident in Figure \ref{fig:weighted_position_vs_model_size}. 
The differences between the GPT-2 based models are insightful, as they share the same architecture and dataset, only differing in the number of parameters.
The larger the model size, the better the ranking results, which aligns with the findings of \cite{radford:2019:language} who showed that the increased model sizes lead to higher effectiveness on various NLP tasks.

With 185 billion parameters, ChatGPT is the largest model in our evaluation, and it ranks best on nearly every query.
For the small to medium-sized fine-tuned models, specifically the 7B variant of Falcon and the 7B and 13B variants of Llama-2, the trend is less clear.
Llama-2 7B is more effective than the 7B variant of Falcon, scoring a mean normalized rank of 0.079 compared to 0.146 by Falcon 7B.
Even though they have the same number of parameters, the differences could be explained by the different training data used for the models and the more sophisticated fine-tuning strategy of Llama-2.

More surprisingly, Llama-2 7B on average ranks about as well as its larger variant, which has nearly twice as many parameters.
The similar rankings indicate either a saturation of the Llama-2 model's effectiveness around that parameter count or that the ranking result is not accurate enough to distinguish between the two models.
Since according to the original paper \cite{touvron:2023:Llama} the model exhibits the usual trend of increasing effectiveness with model size, we assume that the latter is the case.

So, while the model size is a strong indicator of the model's effectiveness in our evaluation, the results do not fully align with the expected trend.

\subsection{Prompting Strategy}
Prompting strategies have a significant impact on the effectiveness of LLMs, shown in Table \ref{tab:prompting_strategy}.
There is a general tendency that the more sophisticated the prompting strategy, the better the ranking results.
While there are two outliers (GPT-2 Medium and GPT-2 Large), which rank better with one of the simpler prompting strategies, the overall trend is clear, with the MultiMedQA prompt leading to the best results.
Having the most complex prompt be most effective aligns with the findings of \cite{reynolds:2021:Prompt} who showed this pattern for the task of text translation using GPT-3.

Even for ChatGPT, which already ranks best on nearly every query without any additional prompting, the ranking results are best when using the complex MultiMedQA prompt.
We assume the small differences between the prompting strategies are because the models are already very effective in answering the queries, so the additional prompting does not have a large impact.

Except for ChatGPT, all models are less effective than the next smaller model, if the smaller model uses the MultiMedQA prompt and the larger model uses no additional prompting.
This gain in effectiveness emphasizes the importance of a good prompting strategy, even for models that are already fine-tuned for a conversational experience like Falcon and Llama-2.

\subsection{Query Type}
Similar to the influence of prompting strategies, our results show that phrasing the query as a proper question leads to better ranking results than using a keyword-based query.
The improvements are consistent across all models, but the difference seems to be less pronounced the larger the model is, as shown in Figure \ref{fig:weighted_position_boxplot_by_model_and_question}.

\subsection{Lower ranked Answers Properties}
When investigating why certain answers by ChatGPT are ranked especially low, we found that the lower-ranked answers follow two patterns.
They either contain the phrase ``As an AI..'' or they contain a formatted list.
For both patterns, we investigated if they are also present in other LLMs and how they are ranked.

The substring ``As an AI..'' is present in answers generated by ChatGPT and by Falcon 7B, but not in any of the other models.
The phrase is commonly used by ChatGPT and seems to be a part of OpenAI's training data, leading the model to not answer controversial questions or telling the user that it does not have enough information to provide an answer.
As Falcon 7B's training data is in part based on self-chats of ChatGPT (see Section \ref{sec:dialog-models}), it is not surprising that the Falcon model also uses this phrase.
Answers containing this substring are ranked lower than other answers, which indicates that the ranking pipeline works as expected in this case, as the answers most likely are not useful for the user.
On the other hand, telling the user that the model is unable to answer the question should be considered a desirable behavior, as the alternative would be to provide a misleading answer.
Future research in rank-based implicit evaluation should investigate how this behavior could be evaluated

Answers exhibiting the list pattern are generated by all models, except for GPT-2 and GPT-2 Medium.
There seems to be a connection between the model size and the frequency of this pattern, as the larger models generate more answers containing lists.
The effect of achieving a worse ranking by using this pattern is not consistent over all models, as the Llama-2 model's answers rank higher when using this pattern.
Additionally, many highly ranked answers by ChatGPT also contain lists, so the pattern is not a clear indicator of a bad answer.
We assume that observing this pattern in many of the lower-ranked answers is dependent on the queries that prompted the answers, which are all keyword-based queries.
Finding more low-ranked answers with the list pattern in keyword-based queries is consistent with our previous finding that keyword-based queries lead to worse ranking results than question-type queries in general.

\subsection{Answer Length}
Figure \ref{fig:weighted_position_vs_answer_length} shows that there is a connection between the length of the generated answer and the ranking result.
Very short and very long answers are ranked worse than answers of medium length containing between 200 and 300 words.
Short answers being ranked worse is consistent with expectations, as the often complex questions require longer answers to be answered sufficiently.
Longer answers being ranked worse is more surprising, but closer inspection revealed that most of the overly long answers are generated by GPT-2 variants, explaining the worse ranking results.
Changing the parameters for text generation using the GPT-2 models could lead to better results, but we did not investigate this further.
\subsection{Summary}
To answer \textbf{RQ1}:
\begin{center}
    \textbf{Which factors influence the effectiveness of LLMs when using rank-based implicit evaluation?}
\end{center}
we investigate the influence of model size, prompting strategy, query type, answer properties and answer length on the effectiveness of LLM generated answers.
We show that model size and prompting strategy have a significant impact on the ranking results, with larger models and more sophisticated prompting strategies leading to better ranking results.
We also show that phrasing the query as a question leads to better ranking results than using a keyword-based query.
Answer length also has an impact on the ranking results, with very short and very long answers being ranked worse than answers of medium length.
Additionally, we investigate the properties of lower-ranked answers but could not find a clear pattern that would explain the worse ranking results.
To summarize, with model size, prompting strategy and query type (which could be considered prompting strategy) we find three factors that directly relate to the model or prompt and have been shown to influence the effectiveness of LLMs in other tasks.


\section{Comparison to Other Benchmarks}
We compare the results of our rank-based implicit evaluation to other benchmarks in the literature, as shown in Table \ref{tab:benchmark_comparison}.
The chosen benchmarks are single-choice question-answering tasks, which as a task is not directly comparable to our rank-based implicit evaluation.
Nevertheless, the order of the models on the benchmark is expected to be similar to the order of the models in our evaluation.
The results of our evaluation are in line with the results of the other benchmarks, with ChatGPT ranking best on all of them.
While all other models are ordered according to their model size, the two versions of Llama-2 are switched in the ranking results of our evaluation compared to the other benchmarks.
We assume that the ranking results of our retrieval pipeline are not accurate enough to distinguish between the two model sizes, either because the retrieval pipeline is not sophisticated enough or because the dataset quality is not high enough both in terms of the web documents quality and the annotations provided by the human annotators.

While ChatGPT ranks best on all benchmarks, our benchmark seems to be less challenging than the other benchmarks, as ChatGPT is already close to the perfect score.
For the other benchmarks, there is still a lot more space for improvement, leaving space for more effective models than ChatGPT.
Our benchmark could not show differences between ChatGPT and models generating even better answers.
To verify if the ChatGPT answers are indeed the best answers, we would need to do additional human evaluation, in which the human annotators would need to compare the highest-ranked LLM-generated answer to the highest-ranked web answer.
If the human annotators prefer the LLM-generated answer, we can conclude that ChatGPT indeed already generates the best answers and that we need a more challenging dataset.
If the human annotators prefer the web answer, we would need to investigate why our retrieval pipeline ranks the LLM-generated answer higher than the web answer and focus on improving the retrieval pipeline in future work.

\subsection{Summary}
Based on the previous discussion we can answer \textbf{RQ2}:
\begin{center}
    \textbf{How does the effectiveness of LLMs on existing benchmarks compare to their effectiveness when using rank-based implicit evaluation?}
\end{center}
by comparing the ranking results of our evaluation to the ranking results of other benchmarks.
We show that the ranking results of our evaluation are in line with the ranking results of other benchmarks, with ChatGPT ranking best on all of them.
However, the ranking results of our evaluation are not accurate enough to distinguish between the two model sizes of Llama-2.
So while the general trends in the effectiveness of LLMs are similar to other benchmarks, there are still deviations from the expected ranking that need to be investigated in future work.
\newpage
\section{Effectiveness of the proposed rank-based implicit evaluation}
Based on the results of our evaluation, we can answer the main research question:
\begin{center}
    \textbf{Is rank-based implicit evaluation with human-written web content a viable approach for evaluating health answers generated by LLMs?}
\end{center}
Our rank-based implicit evaluation method produces ranking results that can be explained by the common factors previously shown to influence the effectiveness of LLMs.
Furthermore, the ranking results are in line with other benchmarks in the literature, suggesting that the proposed method can be an effective way to evaluate LLMs.

However, the ranking results are not accurate enough to distinguish between the two model sizes of Llama-2, which indicates that either the ranking results produced by the retrieval pipeline are not accurate enough or that the deficiencies in the dataset quality in terms of the web documents quality and the annotations provided by the human annotators are too large.
Additionally, ChatGPT already ranks best on nearly every query, so the dataset used in this thesis is not challenging enough to evaluate and compare more advanced models.
Even tough the rank-based implicit evaluation method currently still has its limitations, we believe that it can be a viable approach for evaluating health answers generated by LLMs, especially if the dataset quality is improved and more sophisticated retrieval methods are used.

\section{Limitations}
While the proposed rank-based implicit evaluation method ranks LLMs similarly to other benchmarks in the literature and the ranking results can be explained mostly by the common factors influencing the effectiveness of LLMs, there are several limitations to the method and the evaluation.


\begin{itemize}
    \item 	\textbf{Reliance on Noisy Web Data} The dataset used for evaluation is based on web data, which is not perfectly suited for comparison with answers generated by LLMs. A more suitable dataset would be based on human-generated answers.

    \item 	\textbf{General Answer Quality} In addition to the web data being noisy, manual investigation of the human-written answers shows that the general answer quality is not very high, making the benchmark less suitable for evaluating state-of-the-art models.

    \item 	\textbf{Unreliable Ground Truth} The evaluation assumes that the provided annotations for relevance, credibility and readability are reliable. But as mentioned in Section \ref{sec:documents-with-multiple-ratings}, there are documents rated by multiple annotators for different queries. They often do not agree on the rating, which makes sense for the relevance rating, but not for the credibility and readability ratings. The disagreements indicate that the provided annotations are not as reliable as they could be with a more consistent annotation process.

    \item 	\textbf{Retrieval Pipeline Limitations} While monoT5, the most effective retrieval model in our study, achieves nDCG@10 scores of 0.645 on relevance, 0.72 on credibility and 0.81 on readability there is still much room for improvement. We showed that our evaluation method did not rank the two versions of Llama-2 in order of model size, which we attribute to limitations in the retrieval pipeline. Additionally, the retrieval pipeline is not optimized for the task of retrieving answers to health-related questions, which could be improved by using a more sophisticated retrieval model that is fine-tuned on a health-related dataset.

    \item   \textbf{Bias of Neural Retrievers} Recent work by \cite{dai:2023:llms} has shown that Neural Retrievers like monoT5 are biased towards retrieving documents generated by LLMs compared to human-written documents. While this bias does not change the results for comparing the LLMs against each other, since all of them have the LLM advantage it could still mean that the rankings are higher than they should be. 
\end{itemize}