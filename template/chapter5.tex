\chapter{Discussion \& Limitations}

This section provides an analysis of the experimental setup and results, investigating the quality of the dataset, the performances of various retrieval pipelines, and the capabilities of different LLMs on our dataset.
We will also discuss the limitations and potential biases inherent in our methodologies and results.

\section{RQ1}

\section{RQ2}

\section{Dataset Properties and Challenges}
\begin{itemize}
    \item small dataset, only health
    \item all web data, noisy due to preprocessing
    \item annotations are subjective, inconsistent between mutliple ratings
    \item question type queries probably more relevant for LLMs, keyword based ones could be rewritten
\end{itemize}

\section{Retrieval Pipeline Limitations}
\begin{itemize}
    \item No big improvements of transformer over baseline, QE not making a huge difference
    \item relevance seems to increase the least with more complex model/not much difference between architectures
    \item larger improvements in credibility and readability in transformer models
    \item can monoT5 still be seen as classical retrieval model? There is no indexing or direct comparison of answers, just the calculation of the score based on MS MARCO
    \item not much different from generating scores, like in the fine-tuning step of LLMs
    \item no specific mechanics for evaluating readability/credibility
\end{itemize}


