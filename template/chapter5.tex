
\chapter{Discussion}\label{discussion}

In this chapter, we discuss the results of our experiments and their implications for the research questions. 

\section{RQ1: Factors Influencing LLM Effectiveness}

The effectiveness of LLMs in providing health-related answers varies significantly depending on several factors, as shown by our experimental results.

\subsection{Model Size}
Our results show a strong connection between model size and ranking results in our retrieval-based implicit evaluation as evident in Figure \ref{fig:weighted_position_vs_model_size}. 
Especially the differences between the GPT-2 based models are insightful, as they share the same architecture and dataset, only differing in the number of parameters.
The larger the model size, the better the ranking results, which aligns with the findings of \cite{radford:2019:language} who showed that the increased model sizes leads to better performance on various NLP tasks.

With 185 billion parameters, ChatGPT is the largest model in our evaluation, and it ranks best on nearly every query.
For the small to medium-sized fine-tuned models, specifically the 7B variant of Falcon and the 7B and 13B variants of Llama-2, the trend is less clear.
Llama-2 7B performs better than the 7B variant of Falcon, scoring a mean normalized rank of 0.079 compared to 0.146 by Falcon 7B.
Even though they have the same number of parameters, the differences could be explained by the different training data used for the models and the more sophisticated fine-tuning strategy of Llama-2.

More surprisingly, Llama-2 7B on average ranks about as well as its larger variant, which has nearly twice as many parameters.
This indicates either a saturation of the model's effectiveness or that the ranking result is not accurate enough to distinguish between the two models.
Since according to the original paper \cite{touvron:2023:Llama} the model exhibits the usual trend of increasing performance with model size, we assume that the latter is the case.

So, while the model size is a strong indicator of the model's effectiveness in our evaluation, the results do not fully align with the expected trend.

\subsection{Prompting Strategy}
Prompting strategies have a significant impact on the effectiveness of LLMs, shown in Table \ref{tab:prompting_strategy}.
There is a general tendency that the more sophisticated the prompting strategy, the better the ranking results.
While there are two outliers (GPT-2 Medium and GPT-2 Large), which rank better with one of the simpler prompting strategies, the overall trend is clear, with the MultiMedQA prompt leading to the best results.
This aligns with the findings of \cite{reynolds:2021:Prompt} who showed this pattern for the task of text translation using GPT-3.

Even for ChatGPT, which already ranks best on nearly every query without any additional prompting, the ranking results are best when using the complex MultiMedQA prompt.
We assume the small differences between the prompting strategies are due to the fact that the models are already very effective in answering the queries, so the additional prompting does not have a large impact.

Except for ChatGPT, all models perform worse than the next smaller model, if the smaller models uses the MultiMedQA prompt and the larger model uses no additional prompting.
This emphasizes the importance of a good prompting strategy, even for models that are already fine-tuned for a conversational experience like Falcon and Llama-2.

\subsection{Query Type}
Similar to the influence of prompting strategies, our results show that phrasing the query as a proper question leads to better ranking results than using a keyword-based query.
The improvements are consistent across all models, but the difference seems to be less pronounced the larger the model is, as shown in Figure \ref{fig:weighted_position_boxplot_by_model_and_question}.

\subsection{Lower ranked Answers Properties}
When investigating why certain answers by ChatGPT are ranked especially low, we found that the lower ranked answers follow two patterns.
They either contain the phrase ``As an AI..'' or they contain a formatted list.
For both patterns, we investigated if they are also present in other LLMs and how they are ranked.

The substring ``As an AI..'' is present in answers generated by ChatGPT and by Falcon 7B, but not in any of the other models.
The phrase is commonly used by ChatGPT and seems to be a part of OpenAI's training data, leading the model to not answer controversial questions or telling the user that it does not have enough information to provide an answer.
As Falcon 7B's training data is in part based on self-chats of ChatGPT(see Section \ref{sec:dialog-models}), it is not surprising that it also uses this phrase.
Answers containing this substring are ranked lower than other answers, which indicates that the ranking pipeline works as expected in this case, as the answers most likely are not useful for the user.
On the other hand, telling the user that the model is unable to answer the question should be considered a desirable behavior, as the alternative would be to provide a misleading answer.
Future research in retrieval-based implicit evaluation should investigate how this behavior could be evaluated

Answers exhibiting the list pattern are generated by all models, except for GPT-2 and GPT-2 Medium.
There seems to be a connection between the model size and the frequency of this pattern, as the larger models generate more answers containing lists.
The effect of achieving a worse ranking by using this pattern is not consistent over all models, as the Llama-2 models answers perform better when using this pattern.
Additionally, many highly ranked answers by ChatGPT also contain lists, so the pattern is not a clear indicator of a bad answer.
We assume that observing this pattern in many of the lower ranked answers is dependent on the queries that prompted the answers, which are all keyword-based queries.
This is consistent with the previous finding that keyword-based queries lead to worse ranking results than question-type queries.

\subsection{Answer Length}
Figure \ref{fig:weighted_position_vs_answer_length} shows that there is a connection between the length of the generated answer and the ranking result.
Very short and very long answers are ranked worse than answers of medium length containing between 200 and 300 words.
Short answers being ranked worse is consistent with expectations, as the often complex questions require longer answers to be answered sufficiently.
Longer answers being ranked worse is more surprising, but closer inspection revealed that most of the overly long answers are generated by GPT-2 variants, explaining the worse ranking results.
This could potentially be avoided with other parameters used for the generation, but we did not investigate this further.

\section{RQ2: Comparison to Other Benchmarks}
We compare the results of our retrieval-based implicit evaluation to other benchmarks in the literature, as shown in Table \ref{tab:benchmark_comparison}.
The results of our evaluation are in line with the results of the other benchmarks, with ChatGPT ranking best on all of them.
The only exception are the Llama-2 models, which are switched in the ranking results of our evaluation compared to the other benchmarks.
We discussed this in the previous section when looking at the influence of model size on the ranking results, most likely indicating that the ranking results of our retrieval pipeline not being accurate enough to distinguish between the two model sizes.


\section{Effectiveness of the proposed retrieval-based implicit evaluation}
In conclusion, our retrieval-based implicit evaluation method produces ranking results that are can be explained by the common factors previously shown to influence the effectiveness of LLMs.
Furthermore, the ranking results are in line with other benchmarks in the literature, indicating that the proposed method method can be an effective addition to the evaluation of LLMs.

However, the ranking results are not accurate enough to distinguish between the two model sizes of Llama-2, which indicates that the method is not yet ready to be used as a standalone evaluation method.
Additionally, ChatGPT already ranks best on nearly every query, so the dataset used in this thesis is not challenging enough to evaluate and compare more advanced models.
