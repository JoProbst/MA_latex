
\chapter{Discussion}\label{discussion}

The main goal of this thesis was to investigate whether human-written web content from the medical domain can be used as a proxy for evaluating LLMs in LFQA.
In this chapter, we discuss the results of the underlying research questions that are investigated to answer this overarching question.
Finally, we discuss the limitations of our work.

\section{RQ1: Factors Influencing LLM Effectiveness}

The effectiveness of LLMs in providing health-related answers varies significantly depending on several factors, as shown by our experimental results.

\subsection{Model Size}
Our results show a strong connection between model size and ranking results in our retrieval-based implicit evaluation as evident in Figure \ref{fig:weighted_position_vs_model_size}. 
The differences between the GPT-2 based models are insightful, as they share the same architecture and dataset, only differing in the number of parameters.
The larger the model size, the better the ranking results, which aligns with the findings of \cite{radford:2019:language} who showed that the increased model sizes lead to better performance on various NLP tasks.

With 185 billion parameters, ChatGPT is the largest model in our evaluation, and it ranks best on nearly every query.
For the small to medium-sized fine-tuned models, specifically the 7B variant of Falcon and the 7B and 13B variants of Llama-2, the trend is less clear.
Llama-2 7B performs better than the 7B variant of Falcon, scoring a mean normalized rank of 0.079 compared to 0.146 by Falcon 7B.
Even though they have the same number of parameters, the differences could be explained by the different training data used for the models and the more sophisticated fine-tuning strategy of Llama-2.

More surprisingly, Llama-2 7B on average ranks about as well as its larger variant, which has nearly twice as many parameters.
This indicates either a saturation of the model's effectiveness or that the ranking result is not accurate enough to distinguish between the two models.
Since according to the original paper \cite{touvron:2023:Llama} the model exhibits the usual trend of increasing performance with model size, we assume that the latter is the case.

So, while the model size is a strong indicator of the model's effectiveness in our evaluation, the results do not fully align with the expected trend.

\subsection{Prompting Strategy}
Prompting strategies have a significant impact on the effectiveness of LLMs, shown in Table \ref{tab:prompting_strategy}.
There is a general tendency that the more sophisticated the prompting strategy, the better the ranking results.
While there are two outliers (GPT-2 Medium and GPT-2 Large), which rank better with one of the simpler prompting strategies, the overall trend is clear, with the MultiMedQA prompt leading to the best results.
This aligns with the findings of \cite{reynolds:2021:Prompt} who showed this pattern for the task of text translation using GPT-3.

Even for ChatGPT, which already ranks best on nearly every query without any additional prompting, the ranking results are best when using the complex MultiMedQA prompt.
We assume the small differences between the prompting strategies are because the models are already very effective in answering the queries, so the additional prompting does not have a large impact.

Except for ChatGPT, all models perform worse than the next smaller model, if the smaller model uses the MultiMedQA prompt and the larger model uses no additional prompting.
This emphasizes the importance of a good prompting strategy, even for models that are already fine-tuned for a conversational experience like Falcon and Llama-2.

\subsection{Query Type}
Similar to the influence of prompting strategies, our results show that phrasing the query as a proper question leads to better ranking results than using a keyword-based query.
The improvements are consistent across all models, but the difference seems to be less pronounced the larger the model is, as shown in Figure \ref{fig:weighted_position_boxplot_by_model_and_question}.

\subsection{Lower ranked Answers Properties}
When investigating why certain answers by ChatGPT are ranked especially low, we found that the lower-ranked answers follow two patterns.
They either contain the phrase ``As an AI..'' or they contain a formatted list.
For both patterns, we investigated if they are also present in other LLMs and how they are ranked.

The substring ``As an AI..'' is present in answers generated by ChatGPT and by Falcon 7B, but not in any of the other models.
The phrase is commonly used by ChatGPT and seems to be a part of OpenAI's training data, leading the model to not answer controversial questions or telling the user that it does not have enough information to provide an answer.
As Falcon 7B's training data is in part based on self-chats of ChatGPT(see Section \ref{sec:dialog-models}), it is not surprising that it also uses this phrase.
Answers containing this substring are ranked lower than other answers, which indicates that the ranking pipeline works as expected in this case, as the answers most likely are not useful for the user.
On the other hand, telling the user that the model is unable to answer the question should be considered a desirable behavior, as the alternative would be to provide a misleading answer.
Future research in retrieval-based implicit evaluation should investigate how this behavior could be evaluated

Answers exhibiting the list pattern are generated by all models, except for GPT-2 and GPT-2 Medium.
There seems to be a connection between the model size and the frequency of this pattern, as the larger models generate more answers containing lists.
The effect of achieving a worse ranking by using this pattern is not consistent over all models, as the Llama-2 model's answers perform better when using this pattern.
Additionally, many highly ranked answers by ChatGPT also contain lists, so the pattern is not a clear indicator of a bad answer.
We assume that observing this pattern in many of the lower-ranked answers is dependent on the queries that prompted the answers, which are all keyword-based queries.
This is consistent with the previous finding that keyword-based queries lead to worse ranking results than question-type queries.

\subsection{Answer Length}
Figure \ref{fig:weighted_position_vs_answer_length} shows that there is a connection between the length of the generated answer and the ranking result.
Very short and very long answers are ranked worse than answers of medium length containing between 200 and 300 words.
Short answers being ranked worse is consistent with expectations, as the often complex questions require longer answers to be answered sufficiently.
Longer answers being ranked worse is more surprising, but closer inspection revealed that most of the overly long answers are generated by GPT-2 variants, explaining the worse ranking results.
This could potentially be avoided with other parameters used for the generation, but we did not investigate this further.

\section{RQ2: Comparison to Other Benchmarks}
We compare the results of our retrieval-based implicit evaluation to other benchmarks in the literature, as shown in Table \ref{tab:benchmark_comparison}.
The chosen benchmarks are single-choice question-answering tasks, which as a task is not directly comparable to our retrieval-based implicit evaluation.
Nevertheless, the order of the models on the benchmark is expected to be similar to the order of the models in our evaluation.
The results of our evaluation are in line with the results of the other benchmarks, with ChatGPT ranking best on all of them.
While all other
The only exception are the Llama-2 models, which are switched in the ranking results of our evaluation compared to the other benchmarks.
We discussed this in the previous section when looking at the influence of model size on the ranking results, indicating that the ranking results of our retrieval pipeline are not accurate enough to distinguish between the two model sizes.

While ChatGPT ranks best on all benchmarks, our benchmark seems to be less challenging than the other benchmarks, as ChatGPT is already close to the perfect score.
For the other benchmarks, there is still a lot more space for improvement, leaving space for more sophisticated models to outperform ChatGPT.
Our benchmark could not show differences between ChatGPT and models generating even better answers.

\section{Effectiveness of the proposed retrieval-based implicit evaluation}
In conclusion, our retrieval-based implicit evaluation method produces ranking results that can be explained by the common factors previously shown to influence the effectiveness of LLMs.
Furthermore, the ranking results are in line with other benchmarks in the literature, indicating that the proposed method can be an effective addition to the evaluation of LLMs.

However, the ranking results are not accurate enough to distinguish between the two model sizes of Llama-2, which indicates that the method is not yet ready to be used as a standalone evaluation method.
Additionally, ChatGPT already ranks best on nearly every query, so the dataset used in this thesis is not challenging enough to evaluate and compare more advanced models.
So while the proposed evaluation method can show general trends in the effectiveness of LLMs, it is not yet ready to be used as a standalone benchmark.

\section{Limitations}
While the proposed retrieval-based implicit evaluation method ranks LLMs similarly to other benchmarks in the literature and the ranking results can be explained mostly by the common factors influencing the effectiveness of LLMs, there are several limitations to the method and the evaluation.


\begin{itemize}
    \item 	\textbf{Reliance on Noisy Web Data} The dataset used for evaluation is based on web data, which is not perfectly suited for comparison with answers generated by LLMs. A more suitable dataset would be based on human-generated answers.

    \item 	\textbf{General Answer Quality} In addition to the web data being noisy, manual investigation of the answers shows that the general answer quality is not very high, making the benchmark less suitable for evaluating state-of-the-art models.

    \item 	\textbf{Unreliable Ground Truth} The evaluation assumes that the provided annotations for relevance, credibility and readability are reliable. But as mentioned in Section \ref{sec:documents-with-multiple-ratings}, there are documents rated by multiple annotators for different queries. They often do not agree on the rating, which makes sense for the relevance rating, but not for the credibility and readability ratings. This indicates that the provided annotations are not reliable.

    \item 	\textbf{Retrieval Pipeline Limitations} While monoT5, the best-performing retrieval model in our study, achieves nDCG@10 scores of 0.645 on relevance, 0.72 on credibility and 0.81 on readability there is still much room for improvement. We showed that our evaluation method did not rank the two versions of Llama-2 in order of model size, which we attribute to limitations in the retrieval pipeline. Additionally, the retrieval pipeline is not optimized for the task of retrieving answers to health-related questions, which could be improved by using a more sophisticated retrieval model and a more suitable dataset.

    \item   \textbf{Bias of Neural Retrievers} Recent work by \cite{dai:2023:llms} has shown that Neural Retrievers like monoT5 are biased towards retrieving documents generated by LLMs compared to human-written documents. While this does not change the results for comparing the LLMs against each other, since all of them have the LLM advantage it could still mean that the rankings are higher than they should be. This also implies that we cannot compare the quality of LLM answers to web answers using this method.
\end{itemize}