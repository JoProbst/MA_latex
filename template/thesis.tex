% \documentclass[german,master,buw]{webisthesis} % Weimar
% \documentclass[german,bachelor,fsu]{webisthesis} % Jena
% \documentclass[german,master,ul]{webisthesis} % Leipzig
%
% Non-default programme
% ---------------------
\documentclass[english,master,ul]{webisthesis}\global\thesisprogramme{Data Science M.Sc.}
% \documentclass[english,master,buw]{webisthesis}\global\thesisfrontpagefaculty{Faculty of Civil Engineering/Faculty of Media}\global\thesisprogramme{Digital Engineering}
% \documentclass[german,bachelor,buw]{webisthesis}\global\thesisprogramme{Informatik\\Schwerpunkt Medieninformatik}
% \documentclass[german,bachelor,buw]{webisthesis}\global\thesisprogramme{Informatik\\Schwerpunkt Security and Data Science}
%
% When you change the language, pdflatex may halt on recompilation.
% Just hit enter to continue and recompile again. This should fix it.


%
% Values
% ------
\ThesisSetTitle{Implicit Evaluation of Health Answers from Large Language Models}
\ThesisSetKeywords{LLMs, Health Answers, Implicit Evaluation, Information Retrieval} % only for PDF meta attributes
\ThesisSetLocation{Leipzig}

\ThesisSetAuthor{Jonas Probst}
\ThesisSetStudentNumber{3466651}
\ThesisSetDateOfBirth{10}{09}{1995}
\ThesisSetPlaceOfBirth{Stuttgart}

% Supervisors should usually be Professors from the candidate's university. A second supervisor is not always needed. 
\ThesisSetSupervisors{Prof.\ Dr.\ Martin Potthast,Dr.\ Harrisen Scells}

\ThesisSetSubmissionDate{16}{1}{2024}% TODO Change submission date

% Packages
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning, fit, arrows.meta}
\usepackage{multirow}

% Suggested Packages
% ------------------
\usepackage[sort&compress]{natbib}
%   Allows citing in different ways (e.g., only the authors if you use the
%   citation again within a short time).
%
\usepackage{booktabs}
%    For tables ``looking the right way''.
%
\usepackage{tabularx}
%    Enables tables with columns that automatically fill the page width.
%
% \usepackage[ruled,algochapter]{algorithm2e}
%    A package for pseudo code algorithms.
%
\usepackage{amsmath}
%    For tabular-style formatting of mathematical environments.
%

\usepackage{fontawesome}
%    For lots of awesome glyphs: https://mirror.physik.tu-berlin.de/pub/CTAN/fonts/fontawesome/doc/fontawesome.pdf
%
% Commenting (by your supervisor)
% -------------------------------
\usepackage{xcolor}
\usepackage{soul}
\newcommand{\bscom}[2]{%
  % #1 Original text.
  % #2 Replacement text.
    \st{\scriptsize\,#1}{\color{blue}\scriptsize\,#2}%
  }

% Create links in the pdf document
% Hyperref has some incompatibilities with other packages
% Some other packages must be loaded before, some after hyperref
% Additional options to the hyperref package can be provided in the braces [], like in
% \usehyperref[backref] % This will add back references in the bibliography that some people like ... some don't ... so better ask your supervisor ;-)
\usehyperref
\begin{document}
\begin{frontmatter}
\begin{abstract}
Open-ended generation of text in the form of chatbots is currently one of the biggest use cases of Large Language Models(LLMs), while LLM evaluation still focuses on classical NLP tasks like single choice question answering or text classification, which do not represent the LLMs capabilities to provide long form answers to questions.
The field of long-form question answering(LFQA) is new and previous work points out many problems with the current automated evaluation methods applied for LFQA.
Using human experts to compare generated answers is the gold standard in this space, leading to high cost and slower evaluation procedures while at the same time adding subjectivity to the evaluation.
In this thesis, we present a retrieval-based implicit evaluation method for evaluating long form answers.
Using a dataset of queries and associated documents which were evaluated by human annotators, we first compare multiple retrieval methods for retrieving documents that are relevant, readable and credible.
We then use the most effective retrieval method to rank new answers generated by LLMs against the web answers from the dataset.
Because the retrieval method is previously evaluated to produce rankings similar to the human evaluations, we assume that in ranks the generated LLM answer close to where a human would rank it.
The implicit evaluation method is applied to multiple LLMs of different sizes and trained with different methods.
Answers are generated using different prompting strategies.
We show that our method ranks the evaluated LLMs in a similar order as other benchmarks.
We also show that the LLM ranking improves with model size and with more sophisticated prompting strategies, which is in accordance with other benchmarks.
\end{abstract}

\tableofcontents

% \chapter*{Acknowledgements} % optional
% I thank the authors of the webisthesis template for their excellent work!

% \listoffigures % optional, usually not needed

% \listoftables % optional, usually not needed

% \listofalgorithms % optional, usually not needed
%    requires package algorithm2e

% optional: list of symbols/notation (e.g., using the nomencl package) but usually not needed
\end{frontmatter}

\include{chapter1}
\include{chapter2}
\include{chapter3}
\include{chapter4}
\include{chapter5}
\include{chapter6}

% Bibliography
\bibliographystyle{apalike} % requires package natbib. An alternative is apalike
\bibliography{literature}    % load file literature.bib

\include{appendixA}
\end{document}

