%%% Recommended source for BibTeX entries: https://dblp.org/
%%% Still, you might want to post-process some of the DBLP BibTeX entries ...
%%% ... like changing to titlecase in the title and series fields below and probably removing the DBLP-specific fields timestamp, biburl, and bibsource
@article{akoury:2020:Storium,
  title   = {Storium: A dataset and evaluation platform for machine-in-the-loop story generation},
  author  = {Akoury, Nader and Wang, Shufan and Whiting, Josh and Hood, Stephen and Peng, Nanyun and Iyyer, Mohit},
  journal = {arXiv preprint arXiv:2010.01717},
  year    = {2020}
}
@inproceedings{amati:2006:Frequentist,
  title        = {Frequentist and bayesian approach to information retrieval},
  author       = {Amati, Giambattista},
  booktitle    = {European Conference on Information Retrieval},
  pages        = {13--24},
  year         = {2006},
  organization = {Springer}
}
@article{bajaj:2016:MSMARCO,
  title   = {{MS} {MARCO}: A human generated machine reading comprehension dataset},
  author  = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal = {arXiv preprint arXiv:1611.09268},
  year    = {2016}
}
@inproceedings{banerjee:2005:METEOR,
  title     = {METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author    = {Banerjee, Satanjeev and Lavie, Alon},
  booktitle = {Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages     = {65--72},
  year      = {2005}
}
@misc{beeching:2023:Open,
  author       = {Edward Beeching and Clémentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
  title        = {Open LLM Leaderboard},
  year         = {2023},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}}
}

@article{beltagy:2020:Longformer,
  title   = {Longformer: The long-document transformer},
  author  = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal = {arXiv preprint arXiv:2004.05150},
  year    = {2020}
}
@article{brown:2020:Language,
  author     = {Tom B. Brown and
                Benjamin Mann and
                Nick Ryder and
                Melanie Subbiah and
                Jared Kaplan and
                Prafulla Dhariwal and
                Arvind Neelakantan and
                Pranav Shyam and
                Girish Sastry and
                Amanda Askell and
                Sandhini Agarwal and
                Ariel Herbert{-}Voss and
                Gretchen Krueger and
                Tom Henighan and
                Rewon Child and
                Aditya Ramesh and
                Daniel M. Ziegler and
                Jeffrey Wu and
                Clemens Winter and
                Christopher Hesse and
                Mark Chen and
                Eric Sigler and
                Mateusz Litwin and
                Scott Gray and
                Benjamin Chess and
                Jack Clark and
                Christopher Berner and
                Sam McCandlish and
                Alec Radford and
                Ilya Sutskever and
                Dario Amodei},
  title      = {Language Models are Few-Shot Learners},
  journal    = {CoRR},
  volume     = {abs/2005.14165},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint     = {2005.14165}
}
@article{choi:2018:QuAC,
  title   = {QuAC: Question answering in context},
  author  = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1808.07036},
  year    = {2018}
}

@article{clark:2018:Think,
  title   = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author  = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1803.05457},
  url     = {https://api.semanticscholar.org/CorpusID:3922816}
}


@article{devlin:2018:BERT,
  author     = {Jacob Devlin and
                Ming{-}Wei Chang and
                Kenton Lee and
                Kristina Toutanova},
  title      = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                Understanding},
  journal    = {CoRR},
  volume     = {abs/1810.04805},
  year       = {2018},
  url        = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint     = {1810.04805}
}
@article{dugan:2020:RoFT,
  title   = {RoFT: A tool for evaluating human detection of machine-generated text},
  author  = {Dugan, Liam and Ippolito, Daphne and Kirubarajan, Arun and Callison-Burch, Chris},
  journal = {arXiv preprint arXiv:2010.03070},
  year    = {2020}
}
@inproceedings{duh:2008:Ranking,
  title     = {Ranking vs. regression in machine translation evaluation},
  author    = {Duh, Kevin},
  booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
  pages     = {191--194},
  year      = {2008}
}
@article{ermakova:2022:Overview,
  title    = {Overview of the {CLEF} 2022 {SimpleText} {Task} 2: {Complexity} {Spotting} in {Scientific} {Abstracts}},
  abstract = {This paper provides an overview of the Task 2: What is unclear? of the Automatic Simplification of Scientific Texts (SimpleText) lab, run as part of CLEF 2022. The main aim of the SimpleText lab is to promote a more open scientific information access via automatic text simplification. Task 2 focuses on complexity spotting within scientific texts (passage). Thus, the goal is to detect the terms/concepts that require specific background knowledge for understanding of the passage and to assess their complexity for non-experts. Overall, four runs from four different teams have been submitted to this task. In this paper, we describe the data collection, the task setup, and the evaluation procedure. We also give a brief overview of the participating approaches.},
  language = {en},
  author   = {Ermakova, Liana and Ovchinnikov, Irina and Kamps, Jaap and Nurbakova, Diana and Araújo, Sílvia and Hannachi, Radia},
  keywords = {notion}
}
@article{fan:2019:ELI5,
  title   = {ELI5: Long form question answering},
  author  = {Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal = {arXiv preprint arXiv:1907.09190},
  year    = {2019}
}
@article{freund:2003:An,
  title   = {An efficient boosting algorithm for combining preferences},
  author  = {Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
  journal = {Journal of machine learning research},
  volume  = {4},
  number  = {Nov},
  pages   = {933--969},
  year    = {2003}
}
@inproceedings{goeuriot:2021:Consumer,
  title    = {Consumer {Health} {Search} at {CLEF} {eHealth} 2021},
  url      = {https://www.semanticscholar.org/paper/Consumer-Health-Search-at-CLEF-eHealth-2021-Goeuriot-Suominen/49795bd68af14c6c6970ce68aafcfe9d61a076fa},
  abstract = {This paper details materials, methods, results, and analyses of the Consumer Health Search Task of the CLEF eHealth 2021 Evaluation Lab. This task investigates the effectiveness of information retrieval (IR) approaches in providing access to medical information to laypeople. For this a TREC-style evaluation methodology was applied: a shared collection of documents and queries is distributed, participants’ runs received, relevance assessments generated, and participants’ submissions evaluated. The task generated a new representative web corpus including web pages acquired from a 2021 CommonCrawl and social media content from Twitter and Reddit, along with a new collection of 55 manually generated layperson medical queries and their respective credibility, understandability, and topicality assessments for returned documents. This year’s task focused on three subtask: (i) ad-hoc IR, (ii) weakly supervised IR, and (iii) document credibility prediction. In total, 15 runs were submitted to the three subtasks: eight addressed the ad-hoc IR task, three the weakly supervised IR challenge, and 4 the document credibility prediction challenge. As in previous years, the organizers have made data and tools associated with the task available for future research and development.},
  urldate  = {2023-06-15},
  author   = {Goeuriot, L. and Suominen, H. and Pasi, G. and Bassani, Elias and Brew-Sam, N. and Sáez, Gabriela Nicole González and Kelly, L. and Mulhem, P. and Seneviratne, Sandaru and Upadhyay, Rishabh and Viviani, Marco and Xu, Chenchen},
  year     = {2021},
  keywords = {notion}
}
@article{guzman:2019:Pairwise,
  title   = {Pairwise neural machine translation evaluation},
  author  = {Guzm{\'a}n, Francisco and Joty, Shafiq and M{\`a}rquez, Llu{\'\i}s and Nakov, Preslav},
  journal = {arXiv preprint arXiv:1912.03135},
  year    = {2019}
}
@article{hendrycks:2020:Measuring,
  title   = {Measuring massive multitask language understanding},
  author  = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2009.03300},
  year    = {2020}
}
@misc{imperial:2021:BERT,
  title     = {{BERT} {Embeddings} for {Automatic} {Readability} {Assessment}},
  url       = {http://arxiv.org/abs/2106.07935},
  abstract  = {Automatic readability assessment (ARA) is the task of evaluating the level of ease or difficulty of text documents for a target audience. For researchers, one of the many open problems in the field is to make such models trained for the task show efficacy even for low-resource languages. In this study, we propose an alternative way of utilizing the information-rich embeddings of BERT models with handcrafted linguistic features through a combined method for readability assessment. Results show that the proposed method outperforms classical approaches in readability assessment using English and Filipino datasets, obtaining as high as 12.4\% increase in F1 performance. We also show that the general information encoded in BERT embeddings can be used as a substitute feature set for low-resource languages like Filipino with limited semantic and syntactic NLP tools to explicitly extract feature values for the task.},
  urldate   = {2023-06-17},
  publisher = {arXiv},
  author    = {Imperial, Joseph Marvin},
  month     = jul,
  year      = {2021},
  note      = {arXiv:2106.07935 [cs]},
  keywords  = {Computer Science - Computation and Language, notion}
}
@inproceedings{joachims:2002:Optimizing,
  title     = {Optimizing search engines using clickthrough data},
  author    = {Joachims, Thorsten},
  booktitle = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages     = {133--142},
  year      = {2002}
}
@book{jurafsky:2009:Speech,
  author    = {Dan Jurafsky and
               James H. Martin},
  title     = {Speech and Language Processing: An Introduction to Natural Language
               Processing, Computational Linguistics, and Speech Recognition, 2nd
               Edition},
  series    = {Prentice Hall Series in Artificial Intelligence},
  publisher = {Prentice Hall, Pearson Education International},
  year      = {2009},
  url       = {https://www.worldcat.org/oclc/315913020},
  isbn      = {9780135041963}
}
@inproceedings{khashabi:2018:Looking,
  title     = {Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author    = {Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages     = {252--262},
  year      = {2018}
}
@inproceedings{khattab:2020:Colbert,
  title     = {Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author    = {Khattab, Omar and Zaharia, Matei},
  booktitle = {Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages     = {39--48},
  year      = {2020}
}
@article{kincaid:1975:Derivation,
  title     = {Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author    = {Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year      = {1975},
  publisher = {Institute for Simulation and Training, University of Central Florida}
}
@article{kovcisky:2018:The,
  title     = {The narrativeqa reading comprehension challenge},
  author    = {Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {6},
  pages     = {317--328},
  year      = {2018},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{krishna:2021:Hurdles,
  title   = {Hurdles to progress in long-form question answering},
  author  = {Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  journal = {arXiv preprint arXiv:2103.06332},
  year    = {2021}
}
@article{krishna:2022:Rankgen,
  title   = {Rankgen: Improving text generation with large ranking models},
  author  = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
  journal = {arXiv preprint arXiv:2205.09726},
  year    = {2022}
}
@article{kwiatkowski:2019:Natural,
  title     = {Natural questions: a benchmark for question answering research},
  author    = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {453--466},
  year      = {2019},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{li:2013:Listwise,
  title     = {Listwise approach to learning to rank for automatic evaluation of machine translation},
  author    = {Li, Maoxi and Jiang, Aiwen and Wang, Mingwen},
  booktitle = {Proceedings of Machine Translation Summit XIV: Papers},
  year      = {2013}
}

@inproceedings{lin:2004:Rouge,
  title     = {Rouge: A package for automatic evaluation of summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text summarization branches out},
  pages     = {74--81},
  year      = {2004}
}

@inproceedings{lipani:2017:Fixed,
  title        = {Fixed-cost pooling strategies based on IR evaluation measures},
  author       = {Lipani, Aldo and Palotti, Joao and Lupu, Mihai and Piroi, Florina and Zuccon, Guido and Hanbury, Allan},
  booktitle    = {Advances in Information Retrieval: 39th European Conference on IR Research, ECIR 2017, Aberdeen, UK, April 8-13, 2017, Proceedings 39},
  pages        = {357--368},
  year         = {2017},
  organization = {Springer}
}

@article{macdonald:2012:From,
  title   = {From puppy to maturity: Experiences in developing Terrier},
  author  = {Macdonald, Craig and McCreadie, Richard and Santos, Rodrygo LT and Ounis, Iadh},
  journal = {Proc. of OSIR at SIGIR},
  pages   = {60--63},
  year    = {2012}
}
@book{manning:2001:Foundations,
  author    = {Christopher D. Manning and
               Hinrich Sch{\"{u}}tze},
  title     = {Foundations of Statistical Natural Language Processing},
  publisher = {{MIT} Press},
  year      = {2001},
  isbn      = {978-0-262-13360-9}
}
@book{manning:2009:An,
  title     = {An introduction to information retrieval},
  author    = {Manning, Christopher D},
  year      = {2009},
  publisher = {Cambridge university press}
}
@inproceedings{meel:2020:Web,
  title     = {Web {Text} {Content} {Credibility} {Analysis} using {Max} {Voting} and {Stacking} {Ensemble} {Classifiers}},
  doi       = {10.1109/ACCTHPA49271.2020.9213234},
  abstract  = {The social media has become a great medium for people around the world to openly express their thoughts and views. But for all its advantages, it has also paved way for many people and organizations to intentionally spread fake news and misinform others. And the rate at which fake news is being currently generated, it has become critical to create a reliable mechanism that can efficiently classify a real news from a fake one. This research paper analyses the different approaches, involving ensemble learning, that can be used to accomplish the same by using only text features of the news data. We observe that a combination of three optimal ML algorithms, clubbed by an advanced ensemble learning technique, can give results with an accuracy of more than ninety eight percent.},
  booktitle = {2020 {Advanced} {Computing} and {Communication} {Technologies} for {High} {Performance} {Applications} ({ACCTHPA})},
  author    = {Meel, Priyanka and Chawla, Puneet and Jain, Sahil and Rai, Utkarsh},
  month     = jul,
  year      = {2020},
  keywords  = {Analytical models, Logistics, Machine learning, Measurement, Radio frequency, Stacking, Support vector machines, bernoulli naïve bayes, ensemble, fake news detection, k-nearest neighbors, logistic regression, notion, random forest, svm},
  pages     = {157--161}
}

@article{mihaylov:2018:Can,
  title   = {Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author  = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal = {arXiv preprint arXiv:1809.02789},
  year    = {2018}
}

@article{mitra:2015:CREDBANK,
  title      = {{CREDBANK}: {A} {Large}-{Scale} {Social} {Media} {Corpus} {With} {Associated} {Credibility} {Annotations}},
  volume     = {9},
  copyright  = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
  issn       = {2334-0770},
  shorttitle = {{CREDBANK}},
  url        = {https://ojs.aaai.org/index.php/ICWSM/article/view/14625},
  doi        = {10.1609/icwsm.v9i1.14625},
  abstract   = {Social media has quickly risen to prominence as a news source, yet lingering doubts remain about its ability to spread rumor and misinformation. Systematically studying this phenomenon, however, has been difficult due to the need to collect large-scale, unbiased data along with in-situ judgements of its accuracy. In this paper we present CREDBANK, a corpus designed to bridge this gap by systematically combining machine and human computation.  Specifically, CREDBANK is a corpus of tweets, topics, events and associated human credibility judgements. It is based on the real-time tracking of more than 1 billion streaming tweets over a period of more than three months, computational summarizations of those tweets, and intelligent routings of the tweet streams to human annotators — within a few hours of those events unfolding on Twitter. In total CREDBANK comprises more than 60 million tweets grouped into 1049 real-world events, each annotated by 30 human annotators. As an example, with CREDBANK one can quickly calculate that roughly 24\% of the events in the global tweet stream are not perceived as credible. We have made CREDBANK publicly available, and hope it will enable new research questions related to online information credibility in fields such as social science, data mining and health.},
  language   = {en},
  number     = {1},
  urldate    = {2023-06-17},
  journal    = {Proceedings of the International AAAI Conference on Web and Social Media},
  author     = {Mitra, Tanushree and Gilbert, Eric},
  year       = {2015},
  note       = {Number: 1},
  keywords   = {Micro labor annotations, notion},
  pages      = {258--267}
}

@article{moffat:2008:Rank,
  title     = {Rank-biased precision for measurement of retrieval effectiveness},
  author    = {Moffat, Alistair and Zobel, Justin},
  journal   = {ACM Transactions on Information Systems (TOIS)},
  volume    = {27},
  number    = {1},
  pages     = {1--27},
  year      = {2008},
  publisher = {ACM New York, NY, USA}
}

@article{nakano:2021:Webgpt,
  title   = {Webgpt: Browser-assisted question-answering with human feedback},
  author  = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal = {arXiv preprint arXiv:2112.09332},
  year    = {2021}
}

@article{nogueira:2019:Multi,
  title   = {Multi-stage document ranking with BERT},
  author  = {Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy},
  journal = {arXiv preprint arXiv:1910.14424},
  year    = {2019}
}
@misc{nori:2023:Capabilities,
  title     = {Capabilities of {GPT}-4 on {Medical} {Challenge} {Problems}},
  url       = {http://arxiv.org/abs/2303.13375},
  abstract  = {Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.},
  urldate   = {2023-06-29},
  publisher = {arXiv},
  author    = {Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  month     = apr,
  year      = {2023},
  note      = {arXiv:2303.13375 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion}
}
@misc{openai:2023:GPT,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI},
  year          = {2023},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{ounis:2005:Terrier,
  author    = {Ounis, Iadh and Amati, Gianni and Plachouras, Vassilis and He, Ben and Macdonald, Craig and Johnson, Douglas},
  title     = {Terrier Information Retrieval Platform},
  year      = {2005},
  isbn      = {3540252959},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-540-31865-1_37},
  doi       = {10.1007/978-3-540-31865-1_37},
  abstract  = {Terrier is a modular platform for the rapid development of large-scale Information Retrieval (IR) applications. It can index various document collections, including TREC and Web collections. Terrier also offers a range of document weighting and query expansion models, based on the Divergence From Randomness framework. It has been successfully used for ad-hoc retrieval, cross-language retrieval, Web IR and intranet search, in a centralised or distributed setting.},
  booktitle = {Proceedings of the 27th European Conference on Advances in Information Retrieval Research},
  pages     = {517–519},
  numpages  = {3},
  location  = {Santiago de Compostela, Spain},
  series    = {ECIR'05}
}
@article{ouyang:2022:Training,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {27730--27744},
  year    = {2022}
}

@inproceedings{papineni:2002:Bleu,
  title     = {Bleu: a method for automatic evaluation of machine translation},
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages     = {311--318},
  year      = {2002}
}

@article{pedregosa:2011:Scikit,
  author  = {Fabian Pedregosa and
             Ga{\"{e}}l Varoquaux and
             Alexandre Gramfort and
             Vincent Michel and
             Bertrand Thirion and
             Olivier Grisel and
             Mathieu Blondel and
             Peter Prettenhofer and
             Ron Weiss and
             Vincent Dubourg and
             Jake VanderPlas and
             Alexandre Passos and
             David Cournapeau and
             Matthieu Brucher and
             Matthieu Perrot and
             Edouard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011},
  url     = {http://dl.acm.org/citation.cfm?id=2078195}
}

@article{penedo:2023:The,
  title   = {The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author  = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal = {arXiv preprint arXiv:2306.01116},
  year    = {2023}
}
  
%%% Recommended bibkeys: <last name first author>:<year> 
%%% (in case of a key collision add a, b, c, ... at the end)

%%% Ordering your bibfile by key helps quick browsing w/o need to CTRL+F

@inproceedings{pennington:2014:Glove,
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey  and
               Socher, Richard  and
               Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  pages     = {1532--1543}
}

%%% Another change to DBLP BibTeX entries: spelling out journal names like here
@inproceedings{peters:2018:Deep,
  address   = {New Orleans, Louisiana},
  title     = {Deep {Contextualized} {Word} {Representations}},
  url       = {https://aclanthology.org/N18-1202},
  doi       = {10.18653/v1/N18-1202},
  urldate   = {2023-10-23},
  booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  month     = jun,
  year      = {2018},
  pages     = {2227--2237}
}

%%% When some BibTeX entry is not available at DBLP, try to mimick DBLP's "look and feel" like below
@inproceedings{pyterrier:2020:Declarative,
  author    = {Craig Macdonald and Nicola Tonellotto},
  title     = {Declarative Experimentation inInformation Retrieval using PyTerrier},
  booktitle = {Proceedings of ICTIR 2020},
  year      = {2020}
}

@article{dai:2023:llms,
  title={Llms may dominate information access: Neural retrievers are biased towards llm-generated texts},
  author={Dai, Sunhao and Zhou, Yuqi and Pang, Liang and Liu, Weihao and Hu, Xiaolin and Liu, Yong and Zhang, Xiao and Xu, Jun},
  journal={arXiv preprint arXiv:2310.20501},
  year={2023}
}

@article{radford:2018:Improving,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year      = {2018},
  publisher = {OpenAI}
}
@article{radford:2019:language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{rajpurkar:2016:SQuAD,
  title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
  author    = {Rajpurkar, Pranav  and
               Zhang, Jian  and
               Lopyrev, Konstantin  and
               Liang, Percy},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D16-1264},
  doi       = {10.18653/v1/D16-1264},
  pages     = {2383--2392}
}

@inproceedings{rajpurkar:2018:Know,
  title     = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
  author    = {Rajpurkar, Pranav  and
               Jia, Robin  and
               Liang, Percy},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P18-2124},
  doi       = {10.18653/v1/P18-2124},
  pages     = {784--789},
  abstract  = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.}
}
@inproceedings{ranasinghe:2020:TransQuest,
  address    = {Barcelona, Spain (Online)},
  title      = {{TransQuest}: {Translation} {Quality} {Estimation} with {Cross}-lingual {Transformers}},
  shorttitle = {{TransQuest}},
  url        = {https://aclanthology.org/2020.coling-main.445},
  doi        = {10.18653/v1/2020.coling-main.445},
  abstract   = {Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.},
  urldate    = {2023-06-17},
  booktitle  = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
  publisher  = {International Committee on Computational Linguistics},
  author     = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},
  month      = dec,
  year       = {2020},
  keywords   = {notion},
  pages      = {5070--5081}
}

@inproceedings{reynolds:2021:Prompt,
  title     = {Prompt programming for large language models: Beyond the few-shot paradigm},
  author    = {Reynolds, Laria and McDonell, Kyle},
  booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages     = {1--7},
  year      = {2021}
}

@article{roberts:2019:Exploring,
  title  = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author = {Roberts, Adam and Raffel, Colin and Lee, Katherine and Matena, Michael and Shazeer, Noam and Liu, Peter J and Narang, Sharan and Li, Wei and Zhou, Yanqi},
  year   = {2019}
}

@article{roberts:2020:How,
  author     = {Adam Roberts and
                Colin Raffel and
                Noam Shazeer},
  title      = {How Much Knowledge Can You Pack Into the Parameters of a Language
                Model?},
  journal    = {CoRR},
  volume     = {abs/2002.08910},
  year       = {2020},
  url        = {https://arxiv.org/abs/2002.08910},
  eprinttype = {arXiv},
  eprint     = {2002.08910}
}

@article{robertson:2004:Understanding,
  title     = {Understanding inverse document frequency: on theoretical arguments for IDF},
  author    = {Robertson, Stephen},
  journal   = {Journal of documentation},
  volume    = {60},
  number    = {5},
  pages     = {503--520},
  year      = {2004},
  publisher = {Emerald Group Publishing Limited}
}

@article{rosa:2022:In,
  title   = {In defense of cross-encoders for zero-shot retrieval},
  author  = {Rosa, Guilherme and Bonifacio, Luiz and Jeronymo, Vitor and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Nogueira, Rodrigo},
  journal = {arXiv preprint arXiv:2212.06121},
  year    = {2022}
}

@article{rosenblatt:1958:The,
  author  = {Frank Rosenblatt},
  title   = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  volume  = {65},
  number  = {6},
  pages   = {386--408},
  year    = {1958}
}

@article{sanh:2021:Multitask,
  title   = {Multitask prompted training enables zero-shot task generalization},
  author  = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal = {arXiv preprint arXiv:2110.08207},
  year    = {2021}
}

@article{santhanam:2021:Colbertv2,
  title   = {Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author  = {Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  journal = {arXiv preprint arXiv:2112.01488},
  year    = {2021}
}
}@article{singhal:2022:Large,
  author     = {Karan Singhal and
                Shekoofeh Azizi and
                Tao Tu and
                S. Sara Mahdavi and
                Jason Wei and
                Hyung Won Chung and
                Nathan Scales and
                Ajay Kumar Tanwani and
                Heather Cole{-}Lewis and
                Stephen Pfohl and
                Perry Payne and
                Martin Seneviratne and
                Paul Gamble and
                Chris Kelly and
                Nathaneal Sch{\"{a}}rli and
                Aakanksha Chowdhery and
                Philip Andrew Mansfield and
                Blaise Ag{\"{u}}era y Arcas and
                Dale R. Webster and
                Gregory S. Corrado and
                Yossi Matias and
                Katherine Chou and
                Juraj Gottweis and
                Nenad Tomasev and
                Yun Liu and
                Alvin Rajkomar and
                Joelle K. Barral and
                Christopher Semturs and
                Alan Karthikesalingam and
                Vivek Natarajan},
  title      = {Large Language Models Encode Clinical Knowledge},
  journal    = {CoRR},
  volume     = {abs/2212.13138},
  year       = {2022},
  url        = {https://doi.org/10.48550/arXiv.2212.13138},
  doi        = {10.48550/arXiv.2212.13138},
  eprinttype = {arXiv},
  eprint     = {2212.13138}
}
@article{singhal:2023:Towards,
  title   = {Towards expert-level medical question answering with large language models},
  author  = {Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and others},
  journal = {arXiv preprint arXiv:2305.09617},
  year    = {2023}
}

@article{sparck:1972:A,
  title     = {A statistical interpretation of term specificity and its application in retrieval},
  author    = {Sparck Jones, Karen},
  journal   = {Journal of documentation},
  volume    = {28},
  number    = {1},
  pages     = {11--21},
  year      = {1972},
  publisher = {MCB UP Ltd}
}
@inproceedings{tanaka:2010:Evaluating,
  address   = {New York, NY, USA},
  series    = {{ICUIMC} '10},
  title     = {Evaluating credibility of web information},
  isbn      = {978-1-60558-893-3},
  url       = {https://doi.org/10.1145/2108616.2108645},
  doi       = {10.1145/2108616.2108645},
  abstract  = {We describe a new concept and method for evaluating the Web information credibility. The quality control of information (text, image, video etc.) on the Web is generally insufficient due to low publishing barriers. As a result, there is a large amount of mistaken and unreliable information on the Web that can have detrimental effects on users. This calls for technology that facilitates the judging of the credibility (expertise and trustworthiness) of Web content and the accuracy of the information that users encounter on the Web. Such technology should be able to handle a wide range of tasks: extracting several credibility-related features from the target Web content, extracting reputation-related information for the target Web content, such as hyperlinks and social bookmarks and evaluating its distribution, and evaluating features of the target content authors. We propose and describe methodologies of analyzing information credibility of Web information: (1) content analysis, (2) social support analysis and (3) author analysis. We overview our recent research activities on Web information credibility evaluation based on this methodologies.},
  urldate   = {2023-06-17},
  booktitle = {Proceedings of the 4th {International} {Conference} on {Uniquitous} {Information} {Management} and {Communication}},
  publisher = {Association for Computing Machinery},
  author    = {Tanaka, Katsumi and Ohshima, Hiroaki and Jatowt, Adam and Nakamura, Satoshi and Yamamoto, Yusuke and Sumiya, Kazutoshi and Lee, Ryong and Kitayama, Daisuke and Yumoto, Takayuki and Kawai, Yukiko and Zhang, Jianwei and Nakajima, Shinsuke and Inagaki, Yoichi},
  month     = jan,
  year      = {2010},
  keywords  = {credibility, trustworthiness, web search},
  pages     = {1--10}
}

@article{thakur:2020:Augmented,
  title   = {Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks},
  author  = {Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},
  journal = {arXiv preprint arXiv:2010.08240},
  year    = {2020}
}
@inproceedings{tombros:1998:Advantages,
  title     = {Advantages of query biased summaries in information retrieval},
  author    = {Tombros, Anastasios and Sanderson, Mark},
  booktitle = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages     = {2--10},
  year      = {1998}
}
@article{touvron:2023:Llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}
@article{vaswani:2017:Attention,
  author     = {Ashish Vaswani and
                Noam Shazeer and
                Niki Parmar and
                Jakob Uszkoreit and
                Llion Jones and
                Aidan N. Gomez and
                Lukasz Kaiser and
                Illia Polosukhin},
  title      = {Attention Is All You Need},
  journal    = {CoRR},
  volume     = {abs/1706.03762},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint     = {1706.03762}
}
@inproceedings{vedantam:2015:Cider,
  title     = {Cider: Consensus-based image description evaluation},
  author    = {Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4566--4575},
  year      = {2015}
}
@article{wei:2022:Emergent,
  title   = {Emergent abilities of large language models},
  author  = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal = {arXiv preprint arXiv:2206.07682},
  year    = {2022}
}
@article{bahdanau:2014:neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{khan:2023:chatgpt,
  title={ChatGPT-Reshaping medical education and clinical management},
  author={Khan, Rehan Ahmed and Jawaid, Masood and Khan, Aymen Rehan and Sajjad, Madiha},
  journal={Pakistan Journal of Medical Sciences},
  volume={39},
  number={2},
  pages={605},
  year={2023},
  publisher={Professional Medical Publications}
}
@article{dave:2023:chatgpt,
  title={ChatGPT in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations},
  author={Dave, Tirth and Athaluri, Sai Anirudh and Singh, Satyam},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  pages={1169595},
  year={2023},
  publisher={Frontiers Media SA}
}
@inproceedings{de:2014:seeking,
  title={Seeking and sharing health information online: comparing search engines and social media},
  author={De Choudhury, Munmun and Morris, Meredith Ringel and White, Ryen W},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1365--1376},
  year={2014}
}
@article{xu:2023:A,
  title   = {A critical evaluation of evaluations for long-form question answering},
  author  = {Xu, Fangyuan and Song, Yixiao and Iyyer, Mohit and Choi, Eunsol},
  journal = {arXiv preprint arXiv:2305.18201},
  year    = {2023}
}
@article{xu:2023:Baize,
  title   = {Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
  author  = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal = {arXiv preprint arXiv:2304.01196},
  year    = {2023}
}
@inproceedings{zellers:2019:HellaSwag,
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year      = {2019}
}

@article{zhang:2019:Bertscore,
  title   = {Bertscore: Evaluating text generation with bert},
  author  = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal = {arXiv preprint arXiv:1904.09675},
  year    = {2019}
}

@inproceedings{zhu:2018:Texygen,
  title     = {Texygen: A benchmarking platform for text generation models},
  author    = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle = {The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages     = {1097--1100},
  year      = {2018}
}
@inproceedings{bahri:2021:generative,
  title={Generative models are unsupervised predictors of page quality: A colossal-scale study},
  author={Bahri, Dara and Tay, Yi and Zheng, Che and Brunk, Cliff and Metzler, Donald and Tomkins, Andrew},
  booktitle={Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages={301--309},
  year={2021}
}
@inproceedings{zuccon:2016:Query,
  author    = {Guido Zuccon and
               Jo{\~{a}}o R. M. Palotti and
               Allan Hanbury},
  editor    = {Snehasis Mukhopadhyay and
               ChengXiang Zhai and
               Elisa Bertino and
               Fabio Crestani and
               Javed Mostafa and
               Jie Tang and
               Luo Si and
               Xiaofang Zhou and
               Yi Chang and
               Yunyao Li and
               Parikshit Sondhi},
  title     = {Query Variations and their Effect on Comparing Information Retrieval
               Systems},
  booktitle = {Proceedings of the 25th {ACM} International Conference on Information
               and Knowledge Management, {CIKM} 2016, Indianapolis, IN, USA, October
               24--28, 2016},
  pages     = {691--700},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2983323.2983723},
  doi       = {10.1145/2983323.2983723}
}
