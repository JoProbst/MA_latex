\chapter{Future Work and Conclusion}\label{conclusion}

\section{Future Work}
In this thesis, we present a first approach to using IR methods for evaluating LLMs in LFQA, specifically in the medical domain.
However, there is substantial room for further research and development.
Some areas on which future work could focus are:

\begin{enumerate}
    \item \textbf{Dataset Quality:} The currently used dataset is not originally designed for LFQA and has some limitations, as discussed in Chapter \ref{sec:scope-and-limitations}. Those limitations include the generally lower quality of web content as well as the problems in the annotation process. Future work could focus on designing a more comprehensive dataset that addresses these limitations and provides a more challenging benchmark for LLMs. An intermediate step could be to use the existing dataset and incorporate more sophisticated preprocessing steps to improve the quality of the data. Redoing the current annotations using a more consistent methodology could also improve the dataset quality.
    \item \textbf{Retrieval Pipeline Improvements:} The retrieval pipeline used in this thesis is a first attempt at using IR methods for evaluating LLMs in LFQA. Future work could focus on improving the pipeline's performance and investigate closely in which cases the retrieval pipeline does not align with human annotator preferences. 
    \item \textbf{Multidimensional Evaluation:} The currently used retrieval pipeline is only trained to rank documents based on their relevance, which is only one of the dimensions of answer quality. Future work could focus on extending the pipeline to rank documents based on their readability and credibility.
    \item \textbf{Human Evaluation:} The chosen retrieval pipeline is evaluated on the human-ranked documents from the dataset, but after including the LLM-generated answers the produced ranking is not reevaluated by medical experts. Future work could evaluate if human experts prefer the highest-ranked LLM-generated answers over the highly ranked web answers.
    \item \textbf{Conversational LLMs:} As the current application of LLMs in chatbots is focusing on conversational experiences, it is important to consider the evaluation of these systems, which is more complex than the evaluation of single answers. A first step could be to concatenate the generated answers over multiple conversation turns and rank this against the web document, essentially evaluating the generated text as a whole.
\end{enumerate}

\section{Conclusion}
In this thesis, we present a first approach of using human-written web content as a proxy for evaluating LLMs in LFQA, specifically in the health domain.
We propose a rank-based implicit evaluation method that uses IR methods to rank documents based on their relevance to the question.
Our contributions are:
\begin{enumerate}
    \item Adapting the existing dataset by \cite{goeuriot:2021:Consumer} to the LFQA task.
    \item Implementing and evaluating different retrieval pipelines for the new dataset.
    \item Producing a supplementary dataset of 16,000 answers for the queries in the dataset, using multiple LLMs and prompting strategies.
    \item Evaluating the LFQA performance of the LLMs using the most effective retrieval pipeline.
    \item Investigating which factors influence the ranking of the LLM-generated answers.
    \item Comparing the effectiveness of LLMs on our benchmark to the effectiveness on other benchmarks.
\end{enumerate}

Although the dataset we adapted for this task has several limitations preventing it from being a challenging benchmark that allows for fine-grained evaluation of LLMs, this thesis lays the groundwork for a scalable evaluation method for LFQA.
The proposed rank-based implicit evaluation method is a solid foundation for future work, which could focus on improving the retrieval pipeline, extending the evaluation to other dimensions of answer quality, and creating a more challenging dataset.