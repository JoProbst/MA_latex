\chapter{Future Work and Conclusion}\label{conclusion}

\section{Future Work}
In this thesis we present a first approach of using IR methods for evaluating LLMs in LFQA, specifically in the medical domain.
However, there is substantial room for further research and development.
Some areas on which future work could focus are:

\begin{enumerate}
    \item \textbf{Dataset Improvements:} The currently used dataset is not originally designed for LFQA and has some limitations, as discussed in Chapter \ref{sec:scope-and-limitations}. Future work could focus on designing a more comprehensive dataset that addresses these limitations and provides a more challenging benchmark for LLMs. An intermediate step could be to use the existing dataset and incorporate more sophisticated preprocessing steps to improve the quality of the data. Redoing the current annotations using a more consistent methodology could also improve the dataset quality.
    \item \textbf{Retrieval Pipeline Improvements:} The retrieval pipeline used in this thesis is a first attempt at using IR methods for evaluating LLMs in LFQA. Future work could focus on improving the pipeline's performance and investigate closer in which cases the retrieval pipeline produces different rankings than the human annotators.
    \item \textbf{Multidimensional Evaluation:} The currently used retrieval pipeline is only trained to rank documents based on their relevance, which is only one of the dimensions of answer quality. Future work could focus on extending the pipeline to also rank documents based on their readability and credibility.
    \item \textbf{Professional Evaluation:} The used retrieval pipeline is evaluated on the human ranked documents from the dataset, but after including the LLM generated answers the produced ranking is not evaluated again by medical experts. Future work could evaluate if human experts prefer the highest ranked LLM generated answers over the highly ranked web answers.
    \item \textbf{Conversational LLMs:} As the current application of LLMs in chatbots is focusing on conversational experiences, it is important to already consider the evaluation of these systems, which is more complex than the evaluation of single answers. How could the proposed retrieval-based implicit evaluation be extended to evaluate conversational LLMs?
\end{enumerate}

\section{Conclusion}
The exploration of large language models (LLMs) in the domain of long-form question answering (LFQA), particularly in the medical field, as presented in this thesis, signifies an important step in the broader context of natural language processing and artificial intelligence research.
The innovative use of information retrieval (IR) techniques for the evaluation of LLMs, as detailed in this work, addresses a crucial gap in the existing literature and methodologies.

This thesis, building upon the foundations laid in the introduction and supported by the experimental setup and results, demonstrates that the retrieval-based implicit evaluation method can be a viable tool for assessing the quality of health answers generated by LLMs.
The method's ability to capture the multidimensional nature of answer quality — relevance, readability, and credibility — represents a significant advancement over traditional evaluation methods that often focus on unidimensional metrics.

The research questions, RQ1 and RQ2, provide a structured approach to understanding the factors that influence the effectiveness of LLMs and how these models perform in comparison to established benchmarks in the field.
The findings highlight the importance of model size, prompting strategies, and the formulation of queries in influencing the performance of LLMs.
Moreover, the comparison to other benchmarks reiterates the potential of the proposed method in providing a comprehensive evaluation framework that aligns with the broader trends observed in the field.

However, as discussed in the limitations section, the current study is not without its challenges.
The reliance on noisy web data, the general quality of answers, and the limitations in the retrieval pipeline underscore the need for ongoing improvement and refinement of the methodologies employed.
The scope of the study, primarily focused on English language queries in the medical domain, also suggests the potential for expanding this research to encompass a wider range of languages and fields.

The conclusion of this thesis is not an end, but rather a stepping stone towards a more nuanced and effective evaluation of LLMs in LFQA.
The potential directions for future work, including dataset improvements, enhancement of the retrieval pipeline, and the extension to multidimensional evaluation, pave the way for more sophisticated and accurate assessments of LLMs.
Additionally, the exploration of professional evaluations and the adaptation of this method to conversational LLMs opens up new avenues for research that are more aligned with the current trajectory of LLM applications in real-world scenarios.

In the broader context of AI and NLP research, this thesis contributes to a deeper understanding of the capabilities and limitations of LLMs in handling complex, open-ended queries, particularly in sensitive domains like healthcare.
It underscores the importance of developing robust, multidimensional evaluation methods that can keep pace with the rapid advancements in LLM technology.
As LLMs continue to evolve and find new applications, the methodologies and insights presented in this thesis will serve as a valuable reference point for researchers and practitioners aiming to harness the full potential of these models in an ethical, effective, and user-centric manner.