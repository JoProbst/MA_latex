\chapter{Introduction}\label{structure}

% Health questions often asked on the internet.
% Release of new chatbots like ChatGPT and Bing.
% Fast growing products.
% => will be used for health questions
% Current evaluation techniques for Large Language Models are mostly based on either multipe choice tasks or human evaluation.
% (Cite model announcment paperks and show taksk on which they are evaluated)
% While human evaluation is the most accurate way to evaluate a model, it is also very expensive and time consuming.
% Multiple choice tasks are easier to conduct, but are not really representative of the health question answering task.
% We propose a new metric for evaluating chatbots based on information retrieval techniques.
% We use a dataset from \cite{goeuriot:2021} to evaluate different chatbots.
% We show that our metric is able to capture the number of model parameters.


Recent advancements in Large Language Models (LLMs) have significantly impacted the landscape of Natural Language Processing (NLP), particularly in the realm of information retrieval (IR) and question answering (QA).
The transformative potential of LLMs, especially those based on the transformer architecture introduced by Vaswani et al. (2017), is evident in their ability to comprehend and generate human-like textual responses.
Seminal models like BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018), followed by the larger GPT-3 (Brown et al., 2020), have demonstrated remarkable improvements in various NLP tasks over their predecessors.
These models, trained on extensive datasets, exhibit a wide range of capabilities, including the generation of coherent long texts and performing complex language tasks like translation and summarization.
\\
While the scaling up of models like GPT-3 has led to emergent abilities, as described by Wei et al. (2022), it has also posed new challenges in the evaluation of these LLMs, particularly in the context of QA.
The diversity and complexity of long-form QA, a task where answers are not constrained to predefined formats, necessitates nuanced evaluation methods.
Traditional metrics like accuracy or overlap-based scores are insufficient for comprehensively evaluating such responses.
Instead, human evaluation emerges as the gold standard, examining aspects like relevance, readability, and credibility of the generated answers.
\\
This paper focuses on the intersection of LLMs and IR, specifically in the context of long-form QA.
Leveraging the capabilities of models like GPT-2, Falcon, Llama 2, and ChatGPT, we explore their efficacy in generating responses to health-related queries.
We employ a diverse range of prompting strategies to elicit varied responses from these models, considering the substantial impact of prompting methods on the quality of LLM outputs, as highlighted by Reynolds et al. (2021).

\section{Motivation}\label{sec:motivation}

The motivation behind this thesis stems from the need to develop comprehensive and reliable evaluation methods for LLMs, especially in the realm of long-form QA. Current evaluation paradigms are limited in their ability to fully capture the depth and breadth of responses generated by state-of-the-art LLMs. This limitation is particularly evident when considering the emergent abilities of these models, such as generating coherent long narratives or answering complex queries across various domains (Wei et al., 2022). A robust evaluation framework is crucial for understanding and improving these models, ensuring their reliability, and paving the way for their practical applications in real-world scenarios.

Furthermore, the expanding capabilities of LLMs call for a multi-dimensional approach to evaluation. Aspects such as relevance, readability, and credibility of the responses are as important as their factual accuracy. This multi-faceted approach aligns with the evolving expectations from AI systems in terms of not just providing correct information but also doing so in a manner that is user-friendly and trustworthy.

In summary, this thesis seeks to bridge the gap in the evaluation of LLMs, particularly for long-form QA tasks. By integrating principles from IR and adapting them to the unique challenges posed by LLMs, this work aims to contribute significantly to the field of NLP, offering a comprehensive framework for evaluating the nuanced and complex outputs of contemporary language models.

\section{Research Questions}\label{sec:research-question}


\section{Scope and Limitations}\label{sec:scope-and-limitations}
This thesis is meant as a first step in investigating the use of information retrieval techniques for evaluating Large Language Models.
The used dataset is not originally intended for this purpose and was adapted accordingly.
Limitations of the dataset will be discussed in detail in section TODO.


\section{Structure of the Thesis}\label{sec:structure-of-the-thesis}
The thesis is structured as follows:\\
After this Introduction, the related work will be discussed.
First, the general field of evaluating Large Language Model in the context of question answering is introduced.
Different question answering tasks and their evaluation methods are presented.
Then, the different retrieval methods used in this work are presented, together with the evaluation metrics used to compare them.
A short section on how retrieval methods have previously been used to evaluate NLP tasks is also included.
\\
The next section describes the experimental setting, from dataset collection and preparation over development of the different retrieval pipelines and generation of LLM responses the the questions in the dataset.
\\
Afterwards, the experimental results are presented. The results contain the comparison of the different retrieval methods as well as a ranking of the LLM answers using the best retrieval method.
Additionally, the results of ranking the LLM answers are compared to the results of the current evaluation methods introduced in the related work section.
Noteworthy shortcomings of the LLMS are also discussed in this section.
\\
The thesis is concluded with a discussion and limitations section, followed by a conclusion and an outlook on future work.