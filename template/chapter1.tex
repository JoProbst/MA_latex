\chapter{Introduction}\label{structure}
% In recent years, Large Language Models (LLMs) have become increasingly popular in the field of Natural Language Processing (NLP).
% The development started with the introduction of the attention architecture by \cite{bahdanau:2014:neural}, on which the transformer architecture by \cite{vaswani:2017:Attention} is based.
% The transformer architecture improves upon the previously popular LSTM architecture by allowing the model to factor in more distant dependencies in the text.
% The new architecture also allows for better parallelization of the model training and inference (\cite{vaswani:2017:Attention}).
% Early models based on the transformer architecture such as BERT (\cite{devlin:2018:BERT}) and GPT-2 (\cite{radford:2019:language}) quickly started to outperform previous state-of-the-art models on many NLP tasks.
% With the release of GPT-3 (\cite{brown:2020:Language}), the amount of training data and the number of model parameters were scaled up massively compared to previous models.
% This increase in size allows the model to perform a wide range of tasks, including translation, summarization and question answering (QA).

% Based on this diverse set of capabilities, the models could now be fine-tuned to cater more to the expectations of humans. 
% The idea of Reinforcement Learning from Human Feedback (RLHF) is applied in the release of InstructGPT by \cite{ouyang:2022:Training} to fine-tune a small version of GPT-3, vastly improving the model's output quality based on human evaluations.
% These improvements led to the currently most popular use cases of LLMs: chatbots like OpenAI's ChatGPT\footnote{\url{https://chat.openai.com/}} and Anthropic's Claude\footnote{\url{https://claude.ai/}}.

After the release of ChatGPT\footnote{\url{https://chat.openai.com/}} by OpenAI in 2022 a number of other chatbots like Anthropic's Claude\footnote{\url{https://claude.ai/}} or Google's Bard\footnote{\url{https://bard.google.com/chat}} were published in the following months.
Especially ChatGPT became successful very quickly, gaining 100 million active users in the first year after release, according to the OpenAI DevDay Opening Keynote\footnote{\url{https://www.youtube.com/watch?v=U9mJuUkhUzk}}.
But the chatbots by other companies also got regular updates and a lot of attention from media and the companies, indicating high public interest in this space.
Based on the findings by \cite{de:2014:seeking}, who show that a large amount of users turns to search engines or social media sites for medical information, we can assume that a large portion of users turns to ChatGPT and other chatbots with similar medical questions.
This assumption is supported by studies investigating possible use cases of ChatGPT in the medical domain, which mention the use of ChatGPT by doctors, nursers or medical students as one possible application (\cite{dave:2023:chatgpt}, \cite{khan:2023:chatgpt}).
While multiple benchmarks for answering questions in a single choice format or with short, factual answers are usually reported in the technical reports accompanying the release of new models, it is much harder to evaluate the LLMs on open-ended questions that are common in the medical domain.

As \cite{ouyang:2022:Training} show, the current NLP datasets and benchmarks that are usually applied to evaluate LLMs do not reflect the described use case of answering open-ended questions.
According to their data, only 18\% of GPT-3 API calls are targeted at conventional NLP tasks like classification or single choice QA tasks.
On the other hand, 57\% of API requests lead to open-ended generation.
Considering that this data originates from the GPT-3 API and not the conversation-focused ChatGPT model, it is reasonable to assume that open-ended generation now makes up even more of the requests.
This shows that traditional QA tasks like single or multiple choice QA or extracting answers from a given text are not representative of the task the chatbots are supposed to perform.

The relatively new field of long-form question answering (LFQA) deals with the answering of open-ended questions by automated systems.
As shown by \cite{xu:2023:A}, the current automated methods of evaluating those LFQA systems are still lacking compared to manual human evaluation, which is often performed by crowd workers.
However, \cite{xu:2023:A} also mention multiple drawbacks of the human evaluation approach.
For one, annotators need to be well-trained and have a solid foundational knowledge of the question field.
This level of expertise is usually lacking among crowd workers.
Furthermore, the answer length makes evaluating long-form QA much more demanding than simple QA tasks (\cite{krishna:2021:Hurdles}), increasing the duration it takes to process one sample.
Together, these problems lead to an increased need for well-trained human annotators, which are hard to find and expensive.
Additionally, human evaluations for larger datasets are time-consuming, slowing down the development cycle particularly in the fine-tuning of LLMs.

\cite{krishna:2021:Hurdles} demonstrate a high rate of disagreement among annotators when choosing between two given answers to the same question.
They attribute this to the difficulty of judging answer quality, due to the many factors that make up a good answer.
The subjectivity of the evaluation also poses problems for creating more widely used LFQA benchmarks based on human evaluation.
Even if the questions for all benchmarked models are the same, the subjectivity introduced by human annotators between the different models renders the resulting scores incomparable, assuming not every question is annotated by the same crowd worker for each LLM.

In addition to making the evaluation of correctness more difficult, open-ended generation of text also necessitates that the quality of the answers is evaluated in a multidimensional manner.
In contrast to other QA tasks, LFQA has no simple notion of a good answer since answer quality is not only determined by the correctness of the answer, but also by its readability, relevance and credibility.
Current evaluation methods like accuracy for single choice question answering or the overlap between the extracted answer and the ground truth for extractive question answering are not able to capture the multidimensional nature of answer quality.
To reduce human evaluation costs and enable large-scale, multidimensional evaluation and comparison of chatbots, automated methods are necessary.

In this thesis, we propose a new evaluation method for long-form question answering based on information retrieval techniques.
Using a dataset based on health questions from \cite{goeuriot:2021:Consumer}, we construct a benchmark consisting of multiple queries and human-generated answers based on web content.
We then use retrieval methods to rank answers generated by different LLMs against those human-generated web documents.
Based on the rank of the generated answer, we can compare the quality of the answers by different LLMs.
This method captures the multidimensional nature of answer quality, assuming the retrieval method is effectively ranking documents in terms of relevance, readability and credibility.
Evaluating the multidimensional retrieval performance of different retrieval methods is therefore an important part of this thesis.

With the answer evaluation process being automated, this method allows for large-scale evaluation of LLMs, including the evaluation of different prompting strategies, the assessment of answer consistency across a model, and the analysis of how answer quality varies with the number of model parameters.
This reduction in human evaluations would consequently lower the costs associated with developing and fine-tuning LLMs for LFQA tasks.
It would also lead to more consistency in evaluating generated answers, limiting the subjective component brought in by human annotators.

\section{Research Questions}\label{sec:research-question}
In order to evaluate the proposed retrieval-based implicit evaluation method, we formulate one overarching research question:

\begin{center}
\textbf{Is a retrieval-based implicit evaluation method using human-written web content a viable approach for assessing the quality of health answers generated by LLMs?}
\end{center}

To investigate the main research question, we formulate two supporting research questions:

\begin{itemize}
    \item \textbf{RQ1:} Which factors influence the effectiveness of LLMs on the proposed evaluation method?
    \item \textbf{RQ2:} How does the effectiveness of LLMs on existing benchmarks relate to their effectiveness on the proposed retrieval-based implicit evaluation method?
\end{itemize}


\section{Scope and Limitations}\label{sec:scope-and-limitations}
This thesis is intended as a first step in investigating the use of information retrieval techniques for evaluating the LFQA capabilities of LLMs.
It is not meant to be a comprehensive evaluation of the proposed method, but rather a proof of concept on which future work can build.

The benchmark dataset is not built from scratch, but is based on the dataset from \cite{goeuriot:2021:Consumer}, which is not originally constructed for evaluating LFQA.
Our preprocessing of the web documents still leaves some noise, indicating that more sophisticated methods for extracting the core message from the web documents could yield a more challenging dataset.
The dataset is therefore not perfectly suited for the task, but is still useful for a first evaluation of the proposed method.

Additionally, the dataset is only contains queries and documents in the English language, so the capabilities of the models in other languages are not considered.

The domain of the datasets is restricted to medical queries; other fields are not explored in this work.

\section{Structure of the Thesis}\label{sec:structure-of-the-thesis}
Following this Introduction, in Chapter \ref{related-work} we delve into the related work.
First, the general field of evaluating Large Language Model in the context of question answering is introduced (Section \ref{sec:evaluation-of-large-language-models})
This includes an overview of different question answering tasks and their evaluation methods.

Subsequently, in Section \ref{sec:retrieval-models} the different retrieval methods used in this work are presented, alongside the evaluation metrics used to compare them.
This section is concluded with a discussion on how retrieval methods have previously been used to evaluate NLP tasks.

Chapter \ref{experimental-setup} describes the experimental setting, from dataset collection and preparation over development of the different retrieval pipelines and generation of LLM responses to the questions in the dataset.

The experimental results are presented in Chapter \ref{chapter:results}, aiming to provide the necessary groundwork for discussing the research questions in the following Chapter \ref{discussion}, which is done in the section following the results.

We conclude this thesis with Chapter \ref{conclusion} on the outlook on future work and a conclusion.