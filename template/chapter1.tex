\chapter{Introduction}\label{structure}
In recent years, Large Language Models (LLMs) have become increasingly popular in the field of Natural Language Processing (NLP).
The development started with the introduction of the attention architecture by \cite{vaswani:2017:Attention}, on which the transformer architecture is based.
Early models like BERT (\cite{devlin:2018:BERT}) and GPT-2 (\cite{radford:2018:Improving}) quickly started to outperform previous state-of-the-art models on many NLP tasks.
Scaling up the models and especially the amount of training data led to the release of GPT-3 (\cite{brown:2020:Language}), which is able to perform a wide range of tasks, including translation, summarization and question answering.

The most publicly visible use of LLMs are chatbots, like OpenAI's ChatGPT\footnote{\url{https://chat.openai.com/}} or Anthropic's Claude\footnote{\url{https://claude.ai/}}.
These chatbots are marketed to professionals and casual users alike, with the promise of being able to answer most questions a user could have in a human-like manner.
This is a novel task in the field of NLP, as the chatbots are not constrained to predefined formats, but are able to generate long-form answers.
Traditional question answering tasks like single or choice questions or extracting answers from a given text are not representative of the task the chatbots are supposed to perform.
Therefore, new evaluation methods are needed to evaluate the quality of generated long-form answers.

Furthermore, the quality of the answers has to be evaluated in a multidimensional manner.
In contrast to previous question answering tasks, there is no simple notion of a good answer since answer quality is not only determined by the correctness of the answer, but also by its readability, relevance and credibility.
Current evaluation methods like accuracy for single choice question answering or the overlap between the extracted answer and the ground truth for extractive question answering are not able to capture the multidimensional nature of answer quality.

So far, the most accurate way to evaluate the quality of the answers is human evaluation.
This is however very expensive and time-consuming compared to automated evaluation methods.
To reduce the cost of human evaluation and to be able to evaluate and compare the chatbots on a large scale, automated evaluation methods are needed.

In this thesis, we propose a new evaluation method for long-form question answering based on information retrieval techniques.
Using a dataset based on health questions from \cite{goeuriot:2021:Consumer}, we construct a benchmark consisting of multiple queries and human generated answers based on web content.
We then use retrieval methods to rank answers generated by different LLMs against those human generated answers.
Based on the rank of the generated answer we can compare the quality of the answers by different LLMs.
This method is able to capture the multidimensional nature of answer quality, if the retrieval method is able to capture the relevance, readability and credibility of the answers.
Evaluating the multidimensional retrieval performance of different retrieval methods is therefore an important part of this thesis.

With the answer evaluation process being automated, this method allows for a large scale evaluation of LLMs, including the evaluation of different prompting strategies, the evaluation of how consistent the answers of a model are and the evaluation of how the quality of the answers changes with the number of model parameters.


\section{Research Questions}\label{sec:research-question}
In order to evaluate the proposed retrieval-based implicit evaluation method, we formulate two research questions which serve as the basis for the experiments in this thesis.
Investigating both research questions should help to clarify if the implicit evaluation approach using human written content from the web to distinguish the quality of health answers generated by LLMs is a viable addition to existing evaluation methods.

% dont show dashes in itemize
\begin{itemize}
    \item \textbf{RQ1:} Which factors influence the effectiveness of LLMs on the proposed evaluation method?
    \item \textbf{RQ2:} How does the effectiveness of LLMs on existing benchmarks relate to their effectiveness on the proposed retrieval-based implicit evaluation method?% to they show the same trends? Are they consistent?
\end{itemize}


\section{Scope and Limitations}\label{sec:scope-and-limitations}
This thesis is meant as a first step in investigating the use of information retrieval techniques for evaluating the long-form question answering capabilities of LLMs.
It is not meant to be a comprehensive evaluation of the proposed method, but rather a proof of concept on which future work can build.

The benchmark dataset is not built from scratch, but is based on the dataset from \cite{goeuriot:2021:Consumer}, which is not originally intended for evaluating long-form question answering.
The dataset is therefore not perfectly suited for the task, but is still useful for a first evaluation of the proposed method.


\section{Structure of the Thesis}\label{sec:structure-of-the-thesis}
The thesis is structured as follows:

After this Introduction, the related work will be discussed.
First, the general field of evaluating Large Language Model in the context of question answering is introduced, alongside different question answering tasks and their evaluation methods.
Then, the different retrieval methods used in this work are presented, together with the evaluation metrics used to compare them.
This section is concluded with a discussion on how retrieval methods have previously been used to evaluate NLP tasks.

The next section describes the experimental setting, from dataset collection and preparation over development of the different retrieval pipelines and generation of LLM responses to the questions in the dataset.

Afterwards, the experimental results are presented.
The results contain the comparison of the different retrieval methods as well as a ranking of the LLM answers using the best retrieval method.
We compare different prompting strategies and the correlation of the retrieval-based implicit evaluation method with the number of model parameters.
A comparison of the retrieval-based implicit evaluation method with existing evaluation methods is also included.

The thesis is concluded with a discussion section, followed by outlook on future work and a conclusion.