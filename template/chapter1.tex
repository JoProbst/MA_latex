\chapter{Introduction}\label{structure}

% Health questions often asked on the internet.
% Release of new chatbots like ChatGPT and Bing.
% Fast growing products.
% => will be used for health questions
% Current evaluation techniques for Large Language Models are mostly based on either multipe choice tasks or human evaluation.
% (Cite model announcment paperks and show taksk on which they are evaluated)
% While human evaluation is the most accurate way to evaluate a model, it is also very expensive and time consuming.
% Multiple choice tasks are easier to conduct, but are not really representative of the health question answering task.
% We propose a new metric for evaluating chatbots based on information retrieval techniques.
% We use a dataset from \cite{goeuriot:2021} to evaluate different chatbots.
% We show that our metric is able to capture the number of model parameters.


% Recent advancements in Large Language Models (LLMs) have significantly impacted the landscape of Natural Language Processing (NLP), particularly in the realm of information retrieval (IR) and question answering (QA).
% The transformative potential of LLMs, especially those based on the transformer architecture introduced by Vaswani et al. (2017), is evident in their ability to comprehend and generate human-like textual responses.
% Seminal models like BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018), followed by the larger GPT-3 (Brown et al., 2020), have demonstrated remarkable improvements in various NLP tasks over their predecessors.
% These models, trained on extensive datasets, exhibit a wide range of capabilities, including the generation of coherent long texts and performing complex language tasks like translation and summarization.
% \\
% While the scaling up of models like GPT-3 has led to emergent abilities, as described by Wei et al. (2022), it has also posed new challenges in the evaluation of these LLMs, particularly in the context of QA.
% The diversity and complexity of long-form QA, a task where answers are not constrained to predefined formats, necessitates nuanced evaluation methods.
% Traditional metrics like accuracy or overlap-based scores are insufficient for comprehensively evaluating such responses.
% Instead, human evaluation emerges as the gold standard, examining aspects like relevance, readability, and credibility of the generated answers.
% \\
% This paper focuses on the intersection of LLMs and IR, specifically in the context of long-form QA.
% Leveraging the capabilities of models like GPT-2, Falcon, Llama 2, and ChatGPT, we explore their efficacy in generating responses to health-related queries.
% We employ a diverse range of prompting strategies to elicit varied responses from these models, considering the substantial impact of prompting methods on the quality of LLM outputs, as highlighted by Reynolds et al. (2021).

In recent years, Large Language Models (LLMs) have become increasingly popular in the field of Natural Language Processing (NLP).
The development started with the introduction of the attention architecture by \cite{vaswani:2017:Attention}, on which the transformer architecture is based.
Early models like BERT (\cite{devlin:2018:BERT}) and GPT-2 (\cite{radford:2018:Improving}) quickly started to outperform previous state-of-the-art models on many NLP tasks.
Scaling up the models and especially the amount of training data led to the release of GPT-3 (\cite{brown:2020:Language}), which is able to perform a wide range of tasks, including translation, summarization and question answering.
\\
The most publicly visible use of LLMs are chatbots, like OpenAI's ChatGPT\footnote{\url{https://chat.openai.com/}} or Anthropic's Claude\footnote{\url{https://claude.ai/}}.
These chatbots are marketed to professionals and casual users alike, with the promise of being able to answer most questions a user could have in a human-like manner.
This is a novel task in the field of NLP, as the chatbots are not constrained to predefined formats, but are able to generate long-form answers.
Traditional question answering tasks like single or choice questions or extracting answers from a given text are not representative of the task the chatbots are supposed to perform.
Therefore, new evaluation methods are needed to evaluate the quality of generated long-form answers.
\\
Furthermore, the quality of the answers has to be evaluated in a multidimensional manner.
In contrast to previous question answering tasks, there is no simple notion of a correct answer, since answer quality is not only determined by the correctness of the answer, but also by its readability, relevance and credibility.
This is difficult to capture with traditional evaluation methods like accuracy for single choice question answering or the overlap between the extracted answer and the ground truth for extractive question answering.
\\
Currently, the most accurate way to evaluate the quality of the answers is human evaluation.
This is however very expensive and time-consuming compared to automated evaluation methods.
To reduce the cost of human evaluation and to be able to evaluate and compare the chatbots on a large scale, automated evaluation methods are needed.
\\
In this thesis, we propose a new evaluation method for long-form question answering based on information retrieval techniques.
Using a dataset based on health questions from \cite{goeuriot:2021:Consumer}, we construct a benchmark consisting of multiple queries and human generated answers based on web content.
We then use retrieval methods to rank answers generated by different LLMs against those human generated answers.
Based on the rank of the generated answer we can compare the quality of the answers by different LLMs.
This method is able to capture the multidimensional nature of answer quality, if the retrieval method is able to capture the relevance, readability and credibility of the answers.
Evaluating the multidimensional retrieval performance of different retrieval methods is therefore an important part of this thesis.
\\
With the answer evaluation process being automated, this method allows for a large scale evaluation of LLMs, including the evaluation of different prompting strategies, the evaluation of how consistent the answers of a model are and the evaluation of how the quality of the answers changes with the number of model parameters.

\section{Motivation}\label{sec:motivation}
How to differentiate this section from Introduction?

\section{Research Questions}\label{sec:research-question}
In order to evaluate the proposed retrieval-based implicit evaluation method, the following research questions are investigated:
\begin{itemize}
    \item \textbf{RQ1:} Does the proposed retrieval-based implicit evaluation method correlate with existing evaluation methods?
    \item \textbf{RQ2:} Does the proposed retrieval-based implicit evaluation method correlate with the number of model parameters?
    \item \textbf{RQ3:} Do more complex prompting methods lead to better implicit evaluation scores?
    \item \textbf{RQ4:} Can human written content from the web be used to distinguish the quality of health answers generated by LLMs?
\end{itemize}


\section{Scope and Limitations}\label{sec:scope-and-limitations}
This thesis is meant as a first step in investigating the use of information retrieval techniques for evaluating the long-form question answering capabilities of LLMs.
It is not meant to be a comprehensive evaluation of the proposed method, but rather a proof of concept on which future work can build.
\\
The benchmark dataset is not built from scratch, but is based on the dataset from \cite{goeuriot:2021:Consumer}, which is not originally intended for evaluating long-form question answering.
The dataset is therefore not perfectly suited for the task, but is still useful for a first evaluation of the proposed method.


\section{Structure of the Thesis}\label{sec:structure-of-the-thesis}
The thesis is structured as follows:\\
After this Introduction, the related work will be discussed.
First, the general field of evaluating Large Language Model in the context of question answering is introduced, alongside different question answering tasks and their evaluation methods.
Then, the different retrieval methods used in this work are presented, together with the evaluation metrics used to compare them.
This section is concluded with a discussion on how retrieval methods have previously been used to evaluate NLP tasks.
\\
The next section describes the experimental setting, from dataset collection and preparation over development of the different retrieval pipelines and generation of LLM responses to the questions in the dataset.
\\
Afterwards, the experimental results are presented.
The results contain the comparison of the different retrieval methods as well as a ranking of the LLM answers using the best retrieval method.
We compare different prompting strategies and the correlation of the retrieval-based implicit evaluation method with the number of model parameters.
A comparison of the retrieval-based implicit evaluation method with existing evaluation methods is also included.
\\
The thesis is concluded with a discussion section, followed by outlook on future work and a conclusion.